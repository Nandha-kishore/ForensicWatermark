{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o7LxYZBVUKVT",
        "outputId": "7ef3488c-d048-4991-d2f1-30b2d255b317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-5zyy8d86\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-5zyy8d86\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit ebe8b45437f86395352ab13402ba45b75b4d1ddb\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.4.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.17.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black (from detectron2==0.6)\n",
            "  Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (24.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.3.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.12.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.5)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=6338624 sha256=c2aac51821aff8f273cc6ee33dc55fd56c7c2a8fa050c7c67feed7d4171b3195\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j8ffezx2/wheels/47/e5/15/94c80df2ba85500c5d76599cc307c0a7079d0e221bb6fc4375\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61395 sha256=b2ee54302812f78af4a921dacbfd6711c7953032441218494c316661abcd9dc2\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=6ae0eeb7524404ebe38d5f4686618dfc4bf8d2251e6fc7a8d12ddc277a271364\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-24.8.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-2.10.1 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "68e9d84e7a4447e7b3792404fb937999"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.93-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.19.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.6-py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Downloading ultralytics-8.2.93-py3-none-any.whl (871 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m871.6/871.6 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.6-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.2.93 ultralytics-thop-2.0.6\n",
            "Collecting fiftyone\n",
            "  Downloading fiftyone-0.25.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Collecting aiofiles (from fiftyone)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting argcomplete (from fiftyone)\n",
            "  Downloading argcomplete-3.5.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.12.3)\n",
            "Collecting boto3 (from fiftyone)\n",
            "  Downloading boto3-1.35.19-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.5.0)\n",
            "Collecting dacite<1.8.0,>=1.6.0 (from fiftyone)\n",
            "  Downloading dacite-1.7.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting Deprecated (from fiftyone)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting ftfy (from fiftyone)\n",
            "  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.10.0)\n",
            "Collecting hypercorn>=0.13.2 (from fiftyone)\n",
            "  Downloading hypercorn-0.17.3-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: Jinja2>=3 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.1.4)\n",
            "Collecting kaleido!=0.2.1.post1 (from fiftyone)\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.7.1)\n",
            "Collecting mongoengine==0.24.2 (from fiftyone)\n",
            "  Downloading mongoengine-0.24.2-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting motor>=2.5 (from fiftyone)\n",
            "  Downloading motor-3.5.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fiftyone) (24.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2.1.4)\n",
            "Requirement already satisfied: Pillow>=6.2 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (9.4.0)\n",
            "Requirement already satisfied: plotly>=4.14 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.15.0)\n",
            "Collecting pprintpp (from fiftyone)\n",
            "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.9.5)\n",
            "Collecting pymongo>=3.12 (from fiftyone)\n",
            "  Downloading pymongo-4.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2024.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from fiftyone) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2024.5.15)\n",
            "Collecting retrying (from fiftyone)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.3.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.23.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.13.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fiftyone) (71.0.4)\n",
            "Collecting sseclient-py<2,>=1.7.2 (from fiftyone)\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting sse-starlette<1,>=0.10.3 (from fiftyone)\n",
            "  Downloading sse_starlette-0.10.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting starlette>=0.24.0 (from fiftyone)\n",
            "  Downloading starlette-0.38.5-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting strawberry-graphql==0.138.1 (from fiftyone)\n",
            "  Downloading strawberry_graphql-0.138.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.9.0)\n",
            "Collecting xmltodict (from fiftyone)\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting universal-analytics-python3<2,>=1.0.1 (from fiftyone)\n",
            "  Downloading universal_analytics_python3-1.1.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting pydash (from fiftyone)\n",
            "  Downloading pydash-8.0.3-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting fiftyone-brain<0.18,>=0.17.0 (from fiftyone)\n",
            "  Downloading fiftyone_brain-0.17.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting fiftyone-db<2.0,>=0.4 (from fiftyone)\n",
            "  Downloading fiftyone_db-1.1.5.tar.gz (7.9 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting voxel51-eta<0.13,>=0.12.7 (from fiftyone)\n",
            "  Downloading voxel51_eta-0.12.7-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting graphql-core<3.3.0,>=3.2.0 (from strawberry-graphql==0.138.1->fiftyone)\n",
            "  Downloading graphql_core-3.2.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (2.8.2)\n",
            "Requirement already satisfied: typing_extensions<5.0.0,>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (1.2.2)\n",
            "Collecting h11 (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting h2>=3.1.0 (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting priority (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading priority-2.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting taskgroup (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading taskgroup-0.0.0a4-py2.py3-none-any.whl.metadata (327 bytes)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (2.0.1)\n",
            "Collecting wsproto>=0.14.0 (from hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3->fiftyone) (2.1.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.14->fiftyone) (9.0.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo>=3.12->fiftyone)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette>=0.24.0->fiftyone) (3.7.1)\n",
            "Collecting httpx>=0.10.0 (from universal-analytics-python3<2,>=1.0.1->fiftyone)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting dill (from voxel51-eta<0.13,>=0.12.7->fiftyone)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.13,>=0.12.7->fiftyone) (1.0.0)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.13,>=0.12.7->fiftyone) (0.7)\n",
            "Collecting jsonlines (from voxel51-eta<0.13,>=0.12.7->fiftyone)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting py7zr (from voxel51-eta<0.13,>=0.12.7->fiftyone)\n",
            "  Downloading py7zr-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting rarfile (from voxel51-eta<0.13,>=0.12.7->fiftyone)\n",
            "  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.13,>=0.12.7->fiftyone) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.13,>=0.12.7->fiftyone) (1.16.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.13,>=0.12.7->fiftyone) (2.4.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.13,>=0.12.7->fiftyone) (5.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.13,>=0.12.7->fiftyone) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->fiftyone) (2.6)\n",
            "Collecting botocore<1.36.0,>=1.35.19 (from boto3->fiftyone)\n",
            "  Downloading botocore-1.35.19-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->fiftyone)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->fiftyone)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->fiftyone) (1.16.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->fiftyone) (0.2.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (3.1.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fiftyone) (2024.1)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (3.3)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (2024.8.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (0.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fiftyone) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fiftyone) (3.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (1.3.1)\n",
            "Collecting hyperframe<7,>=6.0 (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->voxel51-eta<0.13,>=0.12.7->fiftyone) (24.2.0)\n",
            "Collecting texttable (from py7zr->voxel51-eta<0.13,>=0.12.7->fiftyone)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting pycryptodomex>=3.16.0 (from py7zr->voxel51-eta<0.13,>=0.12.7->fiftyone)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pyzstd>=0.15.9 (from py7zr->voxel51-eta<0.13,>=0.12.7->fiftyone)\n",
            "  Downloading pyzstd-0.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting pyppmd<1.2.0,>=1.1.0 (from py7zr->voxel51-eta<0.13,>=0.12.7->fiftyone)\n",
            "  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
            "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr->voxel51-eta<0.13,>=0.12.7->fiftyone)\n",
            "  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting multivolumefile>=0.2.3 (from py7zr->voxel51-eta<0.13,>=0.12.7->fiftyone)\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr->voxel51-eta<0.13,>=0.12.7->fiftyone)\n",
            "  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting brotli>=1.1.0 (from py7zr->voxel51-eta<0.13,>=0.12.7->fiftyone)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->voxel51-eta<0.13,>=0.12.7->fiftyone) (3.3.2)\n",
            "Downloading fiftyone-0.25.1-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mongoengine-0.24.2-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading strawberry_graphql-0.138.1-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dacite-1.7.0-py3-none-any.whl (12 kB)\n",
            "Downloading fiftyone_brain-0.17.0-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hypercorn-0.17.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading motor-3.5.1-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-0.10.3-py3-none-any.whl (8.0 kB)\n",
            "Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading starlette-0.38.5-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading universal_analytics_python3-1.1.1-py3-none-any.whl (10 kB)\n",
            "Downloading voxel51_eta-0.12.7-py2.py3-none-any.whl (942 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m942.7/942.7 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading argcomplete-3.5.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.19-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Downloading pydash-8.0.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading botocore-1.35.19-py3-none-any.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.4-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading priority-2.0.0-py3-none-any.whl (8.9 kB)\n",
            "Downloading py7zr-0.22.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rarfile-4.2-py3-none-any.whl (29 kB)\n",
            "Downloading taskgroup-0.0.0a4-py2.py3-none-any.whl (9.1 kB)\n",
            "Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyzstd-0.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.8/413.8 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: fiftyone-db\n",
            "  Building wheel for fiftyone-db (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fiftyone-db: filename=fiftyone_db-1.1.5-py3-none-manylinux1_x86_64.whl size=42156167 sha256=d79d0665f211c0a3b4d0c79b7ab6e28e5aec8d4476ac9e0a8148aaf6462c0d6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/06/80/fc2e6a35e00a5d4de7707afa80ca6336691c5bac41eb6a3dd1\n",
            "Successfully built fiftyone-db\n",
            "Installing collected packages: texttable, sseclient-py, pprintpp, kaleido, brotli, xmltodict, taskgroup, retrying, rarfile, pyzstd, pyppmd, pydash, pycryptodomex, pybcj, priority, multivolumefile, jsonlines, jmespath, inflate64, hyperframe, hpack, h11, graphql-core, ftfy, fiftyone-db, dnspython, dill, Deprecated, dacite, argcomplete, aiofiles, wsproto, strawberry-graphql, starlette, pymongo, py7zr, httpcore, h2, botocore, voxel51-eta, sse-starlette, s3transfer, motor, mongoengine, hypercorn, httpx, fiftyone-brain, universal-analytics-python3, boto3, fiftyone\n",
            "Successfully installed Deprecated-1.2.14 aiofiles-24.1.0 argcomplete-3.5.0 boto3-1.35.19 botocore-1.35.19 brotli-1.1.0 dacite-1.7.0 dill-0.3.8 dnspython-2.6.1 fiftyone-0.25.1 fiftyone-brain-0.17.0 fiftyone-db-1.1.5 ftfy-6.2.3 graphql-core-3.2.4 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.5 httpx-0.27.2 hypercorn-0.17.3 hyperframe-6.0.1 inflate64-1.0.0 jmespath-1.0.1 jsonlines-4.0.0 kaleido-0.2.1 mongoengine-0.24.2 motor-3.5.1 multivolumefile-0.2.3 pprintpp-0.4.0 priority-2.0.0 py7zr-0.22.0 pybcj-1.0.2 pycryptodomex-3.20.0 pydash-8.0.3 pymongo-4.8.0 pyppmd-1.1.0 pyzstd-0.16.1 rarfile-4.2 retrying-1.3.4 s3transfer-0.10.2 sse-starlette-0.10.3 sseclient-py-1.8.0 starlette-0.38.5 strawberry-graphql-0.138.1 taskgroup-0.0.0a4 texttable-1.7.0 universal-analytics-python3-1.1.1 voxel51-eta-0.12.7 wsproto-1.2.0 xmltodict-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!pip install ultralytics\n",
        "!pip install fiftyone opencv-python-headless"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"validation\",\n",
        "    label_types=[\"detections\"],\n",
        "    max_samples=3000,\n",
        "    shuffle=True\n",
        ")# Load 3000 samples from COCO\n",
        "\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))# Set up Detectron2 configuration\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set threshold for this model\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "\n",
        "def runInference(image_path):# Function to run inference on an image\n",
        "    im= cv2.imread(image_path)\n",
        "    outputs = predictor(im)\n",
        "    return outputs, im\n",
        "\n",
        "\n",
        "for sample in dataset.iter_samples():# Run inference and add predictions to the dataset\n",
        "    image_path = sample.filepath\n",
        "    outputs, im = runInference(image_path)\n",
        "\n",
        "\n",
        "    detections = []\n",
        "    for i in range(len(outputs[\"instances\"])):\n",
        "        bbox = outputs[\"instances\"].pred_boxes[i].tensor.cpu().numpy()[0]\n",
        "        score = outputs[\"instances\"].scores[i].cpu().item()\n",
        "        label = MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).thing_classes[outputs[\"instances\"].pred_classes[i]]\n",
        "\n",
        "        detection = fo.Detection(\n",
        "            label=label,\n",
        "            bounding_box=[\n",
        "                bbox[0] / im.shape[1],\n",
        "                bbox[1] / im.shape[0],\n",
        "                (bbox[2] - bbox[0]) / im.shape[1],\n",
        "                (bbox[3] - bbox[1]) / im.shape[0]\n",
        "            ],\n",
        "            confidence=score\n",
        "        )\n",
        "        detections.append(detection)\n",
        "\n",
        "    sample[\"predictions\"] = fo.Detections(detections=detections)\n",
        "    sample.save()\n",
        "\n",
        "\n",
        "results = dataset.evaluate_detections(\n",
        "    \"predictions\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"eval\",\n",
        "    compute_mAP=True\n",
        ")# Evaluate the predictions\n",
        "\n",
        "\n",
        "print(\"\\nDetailed Evaluation Report:\")\n",
        "print(results.report())\n",
        "\n",
        "# From the results we can get the weightes average"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOx-G-f5UWTB",
        "outputId": "3d3b45c1-fc83-4d3d-beea-72d2d93032ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing annotations for 689 downloaded samples to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Writing annotations for 689 downloaded samples to '/root/fiftyone/coco-2017/validation/labels.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/coco-2017/info.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 'coco-2017' split 'validation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'coco-2017' split 'validation'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 100/100 [835.2ms elapsed, 0s remaining, 119.7 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 100/100 [835.2ms elapsed, 0s remaining, 119.7 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'coco-2017-validation-100' created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'coco-2017-validation-100' created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[09/15 08:16:32 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_280758.pkl: 167MB [00:00, 195MB/s]                           \n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating detections...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.eval.detection:Evaluating detections...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 100/100 [3.1s elapsed, 0s remaining, 48.2 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 100/100 [3.1s elapsed, 0s remaining, 48.2 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing IoU sweep...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.eval.coco:Performing IoU sweep...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 100/100 [1.5s elapsed, 0s remaining, 68.9 samples/s]         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 100/100 [1.5s elapsed, 0s remaining, 68.9 samples/s]         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Detailed Evaluation Report:\n",
            "{'apple': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0}, 'backpack': {'precision': 0.375, 'recall': 0.3, 'f1-score': 0.33333333333333326, 'support': 10.0}, 'banana': {'precision': 0.8, 'recall': 1.0, 'f1-score': 0.888888888888889, 'support': 4.0}, 'baseball bat': {'precision': 1.0, 'recall': 0.6, 'f1-score': 0.7499999999999999, 'support': 5.0}, 'baseball glove': {'precision': 0.8, 'recall': 0.6666666666666666, 'f1-score': 0.7272727272727272, 'support': 6.0}, 'bear': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3.0}, 'bed': {'precision': 0.75, 'recall': 1.0, 'f1-score': 0.8571428571428571, 'support': 3.0}, 'bench': {'precision': 0.2, 'recall': 0.2, 'f1-score': 0.20000000000000004, 'support': 5.0}, 'bicycle': {'precision': 0.5833333333333334, 'recall': 0.5, 'f1-score': 0.5384615384615384, 'support': 14.0}, 'bird': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1.0}, 'boat': {'precision': 0.3333333333333333, 'recall': 0.2222222222222222, 'f1-score': 0.26666666666666666, 'support': 9.0}, 'book': {'precision': 0.3125, 'recall': 0.5, 'f1-score': 0.38461538461538464, 'support': 10.0}, 'bottle': {'precision': 0.5909090909090909, 'recall': 0.52, 'f1-score': 0.5531914893617023, 'support': 25.0}, 'bowl': {'precision': 0.6923076923076923, 'recall': 0.6923076923076923, 'f1-score': 0.6923076923076923, 'support': 13.0}, 'broccoli': {'precision': 0.23333333333333334, 'recall': 0.5833333333333334, 'f1-score': 0.33333333333333337, 'support': 12.0}, 'bus': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2.0}, 'cake': {'precision': 0.6923076923076923, 'recall': 0.5, 'f1-score': 0.5806451612903226, 'support': 18.0}, 'car': {'precision': 0.64, 'recall': 0.6956521739130435, 'f1-score': 0.6666666666666666, 'support': 23.0}, 'carrot': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1.0}, 'cat': {'precision': 0.7142857142857143, 'recall': 1.0, 'f1-score': 0.8333333333333333, 'support': 5.0}, 'cell phone': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4.0}, 'chair': {'precision': 0.6666666666666666, 'recall': 0.5517241379310345, 'f1-score': 0.6037735849056604, 'support': 29.0}, 'clock': {'precision': 0.8, 'recall': 0.6666666666666666, 'f1-score': 0.7272727272727272, 'support': 6.0}, 'couch': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2.0}, 'cow': {'precision': 0.2, 'recall': 0.1111111111111111, 'f1-score': 0.14285714285714285, 'support': 9.0}, 'cup': {'precision': 0.6, 'recall': 0.5454545454545454, 'f1-score': 0.5714285714285713, 'support': 11.0}, 'dining table': {'precision': 0.6875, 'recall': 0.4583333333333333, 'f1-score': 0.5499999999999999, 'support': 24.0}, 'dog': {'precision': 0.8333333333333334, 'recall': 1.0, 'f1-score': 0.9090909090909091, 'support': 5.0}, 'donut': {'precision': 0.75, 'recall': 0.375, 'f1-score': 0.5, 'support': 8.0}, 'elephant': {'precision': 0.8333333333333334, 'recall': 0.7142857142857143, 'f1-score': 0.7692307692307692, 'support': 7.0}, 'fire hydrant': {'precision': 1.0, 'recall': 0.8333333333333334, 'f1-score': 0.9090909090909091, 'support': 6.0}, 'fork': {'precision': 0.5714285714285714, 'recall': 0.4444444444444444, 'f1-score': 0.5, 'support': 9.0}, 'frisbee': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0}, 'giraffe': {'precision': 0.7692307692307693, 'recall': 0.7142857142857143, 'f1-score': 0.7407407407407408, 'support': 14.0}, 'handbag': {'precision': 0.45454545454545453, 'recall': 0.43478260869565216, 'f1-score': 0.4444444444444445, 'support': 23.0}, 'horse': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0.0}, 'hot dog': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1.0}, 'keyboard': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2.0}, 'kite': {'precision': 0.3333333333333333, 'recall': 0.2, 'f1-score': 0.25, 'support': 5.0}, 'knife': {'precision': 0.42857142857142855, 'recall': 0.3333333333333333, 'f1-score': 0.375, 'support': 9.0}, 'microwave': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1.0}, 'motorcycle': {'precision': 0.8, 'recall': 0.6666666666666666, 'f1-score': 0.7272727272727272, 'support': 6.0}, 'mouse': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2.0}, 'orange': {'precision': 0.25, 'recall': 0.2, 'f1-score': 0.22222222222222224, 'support': 5.0}, 'oven': {'precision': 0.5, 'recall': 0.16666666666666666, 'f1-score': 0.25, 'support': 6.0}, 'parking meter': {'precision': 0.6666666666666666, 'recall': 0.8, 'f1-score': 0.7272727272727272, 'support': 5.0}, 'person': {'precision': 0.7772727272727272, 'recall': 0.8142857142857143, 'f1-score': 0.7953488372093024, 'support': 210.0}, 'pizza': {'precision': 1.0, 'recall': 0.7142857142857143, 'f1-score': 0.8333333333333333, 'support': 7.0}, 'potted plant': {'precision': 0.5, 'recall': 0.23529411764705882, 'f1-score': 0.31999999999999995, 'support': 17.0}, 'refrigerator': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 2.0}, 'remote': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1.0}, 'sandwich': {'precision': 0.5555555555555556, 'recall': 0.625, 'f1-score': 0.5882352941176471, 'support': 8.0}, 'sheep': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1.0}, 'sink': {'precision': 0.25, 'recall': 0.3333333333333333, 'f1-score': 0.28571428571428575, 'support': 3.0}, 'skateboard': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1.0}, 'skis': {'precision': 0.5714285714285714, 'recall': 0.5714285714285714, 'f1-score': 0.5714285714285714, 'support': 7.0}, 'snowboard': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1.0}, 'spoon': {'precision': 0.25, 'recall': 0.3333333333333333, 'f1-score': 0.28571428571428575, 'support': 3.0}, 'sports ball': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 4.0}, 'stop sign': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1.0}, 'suitcase': {'precision': 0.7142857142857143, 'recall': 1.0, 'f1-score': 0.8333333333333333, 'support': 5.0}, 'surfboard': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 1.0}, 'teddy bear': {'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f1-score': 0.6666666666666666, 'support': 6.0}, 'tennis racket': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2.0}, 'tie': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2.0}, 'toilet': {'precision': 0.6, 'recall': 1.0, 'f1-score': 0.7499999999999999, 'support': 3.0}, 'toothbrush': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2.0}, 'traffic light': {'precision': 0.75, 'recall': 0.5, 'f1-score': 0.6, 'support': 6.0}, 'train': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2.0}, 'truck': {'precision': 0.625, 'recall': 0.7142857142857143, 'f1-score': 0.6666666666666666, 'support': 7.0}, 'tv': {'precision': 0.8, 'recall': 0.5714285714285714, 'f1-score': 0.6666666666666666, 'support': 7.0}, 'umbrella': {'precision': 0.5714285714285714, 'recall': 1.0, 'f1-score': 0.7272727272727273, 'support': 4.0}, 'vase': {'precision': 0.75, 'recall': 1.0, 'f1-score': 0.8571428571428571, 'support': 6.0}, 'wine glass': {'precision': 0.8571428571428571, 'recall': 0.8571428571428571, 'f1-score': 0.8571428571428571, 'support': 7.0}, 'zebra': {'precision': 0.6666666666666666, 'recall': 1.0, 'f1-score': 0.8, 'support': 2.0}, 'micro avg': {'precision': 0.6505139500734214, 'recall': 0.6364942528735632, 'f1-score': 0.643427741466957, 'support': 696.0}, 'macro avg': {'precision': 0.5946760054760055, 'recall': 0.5751033994465161, 'f1-score': 0.5652918613010648, 'support': 696.0}, 'weighted avg': {'precision': 0.6610081377753791, 'recall': 0.6364942528735632, 'f1-score': 0.6374758035411123, 'support': 696.0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"validation\",\n",
        "    label_types=[\"detections\"],\n",
        "    max_samples=1000,\n",
        "    shuffle=True\n",
        ")# Load 1000 samples from COCO dataset\n",
        "\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))# Set up Detectron2 configuration for Faster R-CNN with ResNet-101 backbone\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set threshold for this model\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "\n",
        "def runInference(image_path):\n",
        "    im = cv2.imread(image_path)\n",
        "    outputs = predictor(im)\n",
        "    return outputs, im\n",
        "\n",
        "# Run inference and add predictions to the dataset\n",
        "for sample in dataset.iter_samples():\n",
        "    image_path = sample.filepath\n",
        "    outputs, im = runInference(image_path)\n",
        "\n",
        "    # Convert Detectron2 outputs to FiftyOne format\n",
        "    detections = []\n",
        "    for i in range(len(outputs[\"instances\"])):\n",
        "        bbox = outputs[\"instances\"].pred_boxes[i].tensor.cpu().numpy()[0]\n",
        "        score = outputs[\"instances\"].scores[i].cpu().item()\n",
        "        label = MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).thing_classes[outputs[\"instances\"].pred_classes[i]]\n",
        "\n",
        "        detection = fo.Detection(\n",
        "            label=label,\n",
        "            bounding_box=[\n",
        "                bbox[0] / im.shape[1],\n",
        "                bbox[1] / im.shape[0],\n",
        "                (bbox[2] - bbox[0]) / im.shape[1],\n",
        "                (bbox[3] - bbox[1]) / im.shape[0]\n",
        "            ],\n",
        "            confidence=score\n",
        "        )\n",
        "        detections.append(detection)\n",
        "\n",
        "    sample[\"predictions\"] = fo.Detections(detections=detections)\n",
        "    sample.save()\n",
        "\n",
        "\n",
        "results = dataset.evaluate_detections(\n",
        "    \"predictions\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"eval\",\n",
        "    compute_mAP=True\n",
        ")# Evaluate the predictions\n",
        "\n",
        "print(\"\\nDetailed Evaluation Report:\")\n",
        "print(results.report())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLR8ncnbcWdd",
        "outputId": "3fc0bbb6-c674-433a-c1d9-6fb8de9e550b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only found 149 (<3000) samples matching your requirements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fiftyone.utils.coco:Only found 149 (<3000) samples matching your requirements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing dataset 'coco-2017-validation-3000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading existing dataset 'coco-2017-validation-3000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[09/04 14:31:09 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_101_FPN_3x/137851257/model_final_f6e8b1.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_f6e8b1.pkl: 243MB [00:00, 246MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating detections...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.eval.detection:Evaluating detections...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 149/149 [9.5s elapsed, 0s remaining, 11.8 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 149/149 [9.5s elapsed, 0s remaining, 11.8 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing IoU sweep...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.eval.coco:Performing IoU sweep...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 149/149 [4.5s elapsed, 0s remaining, 36.9 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 149/149 [4.5s elapsed, 0s remaining, 36.9 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Detailed Evaluation Report:\n",
            "{'airplane': {'precision': 0.8, 'recall': 0.8, 'f1-score': 0.8000000000000002, 'support': 5}, 'apple': {'precision': 0.3333333333333333, 'recall': 0.5, 'f1-score': 0.4, 'support': 6}, 'backpack': {'precision': 0.5517241379310345, 'recall': 0.2909090909090909, 'f1-score': 0.38095238095238093, 'support': 55}, 'banana': {'precision': 0.9, 'recall': 0.4090909090909091, 'f1-score': 0.5625000000000001, 'support': 22}, 'baseball bat': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'bed': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'bench': {'precision': 0.55, 'recall': 0.3793103448275862, 'f1-score': 0.4489795918367347, 'support': 29}, 'bicycle': {'precision': 0.7415254237288136, 'recall': 0.54858934169279, 'f1-score': 0.6306306306306306, 'support': 319}, 'bird': {'precision': 1.0, 'recall': 0.2, 'f1-score': 0.33333333333333337, 'support': 15}, 'boat': {'precision': 0.7, 'recall': 0.65625, 'f1-score': 0.6774193548387096, 'support': 32}, 'book': {'precision': 0.7777777777777778, 'recall': 0.3181818181818182, 'f1-score': 0.45161290322580644, 'support': 22}, 'bottle': {'precision': 0.5, 'recall': 0.6363636363636364, 'f1-score': 0.56, 'support': 11}, 'bowl': {'precision': 0.16666666666666666, 'recall': 1.0, 'f1-score': 0.2857142857142857, 'support': 1}, 'broccoli': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'bus': {'precision': 0.7714285714285715, 'recall': 0.7941176470588235, 'f1-score': 0.782608695652174, 'support': 34}, 'cake': {'precision': 1.0, 'recall': 0.23076923076923078, 'f1-score': 0.375, 'support': 13}, 'car': {'precision': 0.7603686635944701, 'recall': 0.7783018867924528, 'f1-score': 0.7692307692307692, 'support': 212}, 'cat': {'precision': 1.0, 'recall': 0.6666666666666666, 'f1-score': 0.8, 'support': 3}, 'cell phone': {'precision': 0.6666666666666666, 'recall': 0.25, 'f1-score': 0.36363636363636365, 'support': 8}, 'chair': {'precision': 0.45454545454545453, 'recall': 0.29850746268656714, 'f1-score': 0.36036036036036034, 'support': 67}, 'clock': {'precision': 0.5, 'recall': 0.8, 'f1-score': 0.6153846153846154, 'support': 5}, 'couch': {'precision': 0.5, 'recall': 0.2, 'f1-score': 0.28571428571428575, 'support': 5}, 'cow': {'precision': 0.45454545454545453, 'recall': 0.5555555555555556, 'f1-score': 0.5, 'support': 9}, 'cup': {'precision': 0.8333333333333334, 'recall': 0.7142857142857143, 'f1-score': 0.7692307692307692, 'support': 14}, 'dining table': {'precision': 0.4444444444444444, 'recall': 0.2857142857142857, 'f1-score': 0.34782608695652173, 'support': 14}, 'dog': {'precision': 0.6428571428571429, 'recall': 0.75, 'f1-score': 0.6923076923076924, 'support': 12}, 'fire hydrant': {'precision': 0.8333333333333334, 'recall': 0.7142857142857143, 'f1-score': 0.7692307692307692, 'support': 7}, 'handbag': {'precision': 0.5128205128205128, 'recall': 0.2857142857142857, 'f1-score': 0.36697247706422015, 'support': 70}, 'horse': {'precision': 0.8, 'recall': 0.6666666666666666, 'f1-score': 0.7272727272727272, 'support': 6}, 'hot dog': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 6}, 'kite': {'precision': 0.3333333333333333, 'recall': 1.0, 'f1-score': 0.5, 'support': 1}, 'knife': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'laptop': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'motorcycle': {'precision': 0.7413793103448276, 'recall': 0.5584415584415584, 'f1-score': 0.6370370370370371, 'support': 77}, 'orange': {'precision': 1.0, 'recall': 0.2, 'f1-score': 0.33333333333333337, 'support': 5}, 'parking meter': {'precision': 0.75, 'recall': 0.6, 'f1-score': 0.6666666666666665, 'support': 5}, 'person': {'precision': 0.8641304347826086, 'recall': 0.8377239199157007, 'f1-score': 0.8507223113964687, 'support': 949}, 'pizza': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'potted plant': {'precision': 0.35294117647058826, 'recall': 0.4, 'f1-score': 0.37500000000000006, 'support': 15}, 'refrigerator': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'remote': {'precision': 0.4, 'recall': 0.5, 'f1-score': 0.4444444444444445, 'support': 4}, 'sandwich': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'scissors': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'skateboard': {'precision': 0.7894736842105263, 'recall': 0.7142857142857143, 'f1-score': 0.7500000000000001, 'support': 21}, 'spoon': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'sports ball': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'stop sign': {'precision': 0.6666666666666666, 'recall': 0.5, 'f1-score': 0.5714285714285715, 'support': 4}, 'suitcase': {'precision': 0.7272727272727273, 'recall': 0.47058823529411764, 'f1-score': 0.5714285714285714, 'support': 17}, 'surfboard': {'precision': 0.8888888888888888, 'recall': 0.6666666666666666, 'f1-score': 0.761904761904762, 'support': 12}, 'teddy bear': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'tie': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'traffic light': {'precision': 0.6222222222222222, 'recall': 0.56, 'f1-score': 0.5894736842105264, 'support': 100}, 'train': {'precision': 0.6363636363636364, 'recall': 0.7777777777777778, 'f1-score': 0.7000000000000001, 'support': 9}, 'truck': {'precision': 0.5714285714285714, 'recall': 0.5925925925925926, 'f1-score': 0.5818181818181818, 'support': 27}, 'tv': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}, 'umbrella': {'precision': 0.5833333333333334, 'recall': 0.6363636363636364, 'f1-score': 0.6086956521739131, 'support': 33}, 'vase': {'precision': 0.4, 'recall': 0.25, 'f1-score': 0.3076923076923077, 'support': 8}, 'micro avg': {'precision': 0.7555555555555555, 'recall': 0.6626245127760936, 'f1-score': 0.7060452238117212, 'support': 2309}, 'macro avg': {'precision': 0.5618035947776305, 'recall': 0.47357404137893966, 'f1-score': 0.4819689523293789, 'support': 2309}, 'weighted avg': {'precision': 0.7524842352905603, 'recall': 0.6626245127760936, 'f1-score': 0.6952797693623719, 'support': 2309}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"validation\",\n",
        "    label_types=[\"detections\"],\n",
        "    max_samples=1000,\n",
        "    shuffle=True\n",
        ")# Load 1000 samples from COCO dataset\n",
        "\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))# Set up Detectron2 configuration for Mask R-CNN\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set threshold for this model\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "\n",
        "def runInference(image_path):# Function to run inference on an image\n",
        "    im = cv2.imread(image_path)\n",
        "    outputs = predictor(im)\n",
        "    return outputs, im\n",
        "\n",
        "\n",
        "for sample in dataset.iter_samples():\n",
        "    image_path = sample.filepath\n",
        "    outputs, im = runInference(image_path)\n",
        "\n",
        "\n",
        "    detections = []\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    for i in range(len(instances)):\n",
        "        bbox = instances.pred_boxes[i].tensor.numpy()[0]\n",
        "        score = instances.scores[i].item()\n",
        "        label = MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).thing_classes[instances.pred_classes[i].item()]\n",
        "\n",
        "        # If masks are present, extract and process them\n",
        "        mask = instances.pred_masks[i].numpy().astype(np.uint8) * 255\n",
        "        mask = cv2.resize(mask, (im.shape[1], im.shape[0]))  # Resize mask to match image dimensions\n",
        "\n",
        "        detection = fo.Detection(\n",
        "            label=label,\n",
        "            bounding_box=[\n",
        "                bbox[0] / im.shape[1],\n",
        "                bbox[1] / im.shape[0],\n",
        "                (bbox[2] - bbox[0]) / im.shape[1],\n",
        "                (bbox[3] - bbox[1]) / im.shape[0]\n",
        "            ],\n",
        "            confidence=score,\n",
        "            mask=mask\n",
        "        )\n",
        "        detections.append(detection)\n",
        "\n",
        "    sample[\"predictions\"] = fo.Detections(detections=detections)\n",
        "    sample.save()\n",
        "\n",
        "\n",
        "results = dataset.evaluate_detections(\n",
        "    \"predictions\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"eval\",\n",
        "    compute_mAP=True\n",
        ")# Evaluate the predictions\n",
        "\n",
        "\n",
        "print(\"\\nDetailed Evaluation Report:\")\n",
        "print(results.report())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouXQTa33bM2i",
        "outputId": "1f7c6852-da89-4c10-efc5-a6670461af63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only found 149 (<3000) samples matching your requirements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fiftyone.utils.coco:Only found 149 (<3000) samples matching your requirements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing dataset 'coco-2017-validation-3000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading existing dataset 'coco-2017-validation-3000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[09/04 14:25:18 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_f10217.pkl: 178MB [00:00, 235MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating detections...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.eval.detection:Evaluating detections...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 149/149 [19.0s elapsed, 0s remaining, 5.4 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 149/149 [19.0s elapsed, 0s remaining, 5.4 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing IoU sweep...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.eval.coco:Performing IoU sweep...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 149/149 [6.4s elapsed, 0s remaining, 22.7 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 149/149 [6.4s elapsed, 0s remaining, 22.7 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Average Precision (mAP):\n",
            "0.2963747632673844\n",
            "\n",
            "Detailed Evaluation Report:\n",
            "{'airplane': {'precision': 0.5, 'recall': 0.8, 'f1-score': 0.6153846153846154, 'support': 5}, 'apple': {'precision': 0.6666666666666666, 'recall': 0.3333333333333333, 'f1-score': 0.4444444444444444, 'support': 6}, 'backpack': {'precision': 0.6956521739130435, 'recall': 0.2909090909090909, 'f1-score': 0.41025641025641024, 'support': 55}, 'banana': {'precision': 1.0, 'recall': 0.2857142857142857, 'f1-score': 0.4444444444444445, 'support': 21}, 'bed': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'bench': {'precision': 0.5789473684210527, 'recall': 0.3793103448275862, 'f1-score': 0.45833333333333337, 'support': 29}, 'bicycle': {'precision': 0.7188755020080321, 'recall': 0.5628930817610063, 'f1-score': 0.6313932980599647, 'support': 318}, 'bird': {'precision': 0.75, 'recall': 0.2, 'f1-score': 0.31578947368421056, 'support': 15}, 'boat': {'precision': 0.75, 'recall': 0.65625, 'f1-score': 0.7, 'support': 32}, 'book': {'precision': 0.75, 'recall': 0.14285714285714285, 'f1-score': 0.24, 'support': 21}, 'bottle': {'precision': 0.5, 'recall': 0.7272727272727273, 'f1-score': 0.5925925925925926, 'support': 11}, 'bowl': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'broccoli': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'bus': {'precision': 0.6666666666666666, 'recall': 0.7647058823529411, 'f1-score': 0.7123287671232877, 'support': 34}, 'cake': {'precision': 1.0, 'recall': 0.15384615384615385, 'f1-score': 0.2666666666666667, 'support': 13}, 'car': {'precision': 0.7288135593220338, 'recall': 0.8, 'f1-score': 0.7627494456762749, 'support': 215}, 'cat': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 3}, 'cell phone': {'precision': 1.0, 'recall': 0.25, 'f1-score': 0.4, 'support': 8}, 'chair': {'precision': 0.5319148936170213, 'recall': 0.373134328358209, 'f1-score': 0.4385964912280702, 'support': 67}, 'clock': {'precision': 0.8, 'recall': 0.8, 'f1-score': 0.8000000000000002, 'support': 5}, 'couch': {'precision': 1.0, 'recall': 0.2, 'f1-score': 0.33333333333333337, 'support': 5}, 'cow': {'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f1-score': 0.6666666666666666, 'support': 9}, 'cup': {'precision': 0.75, 'recall': 0.6428571428571429, 'f1-score': 0.6923076923076924, 'support': 14}, 'dining table': {'precision': 0.375, 'recall': 0.21428571428571427, 'f1-score': 0.2727272727272727, 'support': 14}, 'dog': {'precision': 0.5714285714285714, 'recall': 0.6666666666666666, 'f1-score': 0.6153846153846153, 'support': 12}, 'fire hydrant': {'precision': 0.5714285714285714, 'recall': 0.5714285714285714, 'f1-score': 0.5714285714285714, 'support': 7}, 'frisbee': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'handbag': {'precision': 0.39473684210526316, 'recall': 0.21428571428571427, 'f1-score': 0.27777777777777773, 'support': 70}, 'horse': {'precision': 0.5714285714285714, 'recall': 0.6666666666666666, 'f1-score': 0.6153846153846153, 'support': 6}, 'hot dog': {'precision': 1.0, 'recall': 0.16666666666666666, 'f1-score': 0.2857142857142857, 'support': 6}, 'kite': {'precision': 0.25, 'recall': 1.0, 'f1-score': 0.4, 'support': 1}, 'knife': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'laptop': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'motorcycle': {'precision': 0.6933333333333334, 'recall': 0.6582278481012658, 'f1-score': 0.6753246753246753, 'support': 79}, 'orange': {'precision': 1.0, 'recall': 0.2, 'f1-score': 0.33333333333333337, 'support': 5}, 'parking meter': {'precision': 0.3333333333333333, 'recall': 0.2, 'f1-score': 0.25, 'support': 5}, 'person': {'precision': 0.8343685300207039, 'recall': 0.8378378378378378, 'f1-score': 0.8360995850622407, 'support': 962}, 'pizza': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'potted plant': {'precision': 0.4, 'recall': 0.5333333333333333, 'f1-score': 0.4571428571428572, 'support': 15}, 'refrigerator': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'remote': {'precision': 0.5, 'recall': 0.75, 'f1-score': 0.6, 'support': 4}, 'sandwich': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'scissors': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'sheep': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'sink': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'skateboard': {'precision': 0.875, 'recall': 0.6666666666666666, 'f1-score': 0.7567567567567567, 'support': 21}, 'spoon': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'sports ball': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'stop sign': {'precision': 0.6666666666666666, 'recall': 0.5, 'f1-score': 0.5714285714285715, 'support': 4}, 'suitcase': {'precision': 0.6, 'recall': 0.35294117647058826, 'f1-score': 0.4444444444444445, 'support': 17}, 'surfboard': {'precision': 1.0, 'recall': 0.5833333333333334, 'f1-score': 0.7368421052631579, 'support': 12}, 'teddy bear': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 1}, 'tennis racket': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'tie': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2}, 'traffic light': {'precision': 0.686046511627907, 'recall': 0.5959595959595959, 'f1-score': 0.6378378378378378, 'support': 99}, 'train': {'precision': 0.8571428571428571, 'recall': 0.6666666666666666, 'f1-score': 0.75, 'support': 9}, 'truck': {'precision': 0.6153846153846154, 'recall': 0.5925925925925926, 'f1-score': 0.6037735849056604, 'support': 27}, 'tv': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}, 'umbrella': {'precision': 0.6285714285714286, 'recall': 0.6666666666666666, 'f1-score': 0.6470588235294118, 'support': 33}, 'vase': {'precision': 0.5, 'recall': 0.25, 'f1-score': 0.3333333333333333, 'support': 8}, 'micro avg': {'precision': 0.7419818094782192, 'recall': 0.667240637107189, 'f1-score': 0.7026291931097008, 'support': 2323}, 'macro avg': {'precision': 0.5413012221625502, 'recall': 0.4263995983064139, 'f1-score': 0.4377291231441349, 'support': 2323}, 'weighted avg': {'precision': 0.7418877531573754, 'recall': 0.667240637107189, 'f1-score': 0.6886777897364388, 'support': 2323}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"validation\",\n",
        "    label_types=[\"detections\"],\n",
        "    max_samples=3000,\n",
        "    shuffle=True\n",
        ")# Load 3000 samples from COCO dataset\n",
        "\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))# Set up Detectron2 configuration for Mask R-CNN with ResNet-101 backbone\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set threshold for this model\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "\n",
        "def runInference(image_path):\n",
        "    im = cv2.imread(image_path)\n",
        "    outputs = predictor(im)\n",
        "    return outputs, im\n",
        "\n",
        "# Run inference and add predictions to the dataset\n",
        "for sample in dataset.iter_samples():\n",
        "    image_path = sample.filepath\n",
        "    outputs, im = runInference(image_path)\n",
        "\n",
        "    # Convert Detectron2 outputs to FiftyOne format\n",
        "    detections = []\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "    for i in range(len(instances)):\n",
        "        bbox = instances.pred_boxes[i].tensor.numpy()[0]\n",
        "        score = instances.scores[i].item()\n",
        "        label = MetadataCatalog.get(cfg.DATASETS.TRAIN[0]).thing_classes[instances.pred_classes[i].item()]\n",
        "\n",
        "        # If masks are present, extract and process them\n",
        "        mask = instances.pred_masks[i].numpy().astype(np.uint8) * 255\n",
        "        mask = cv2.resize(mask, (im.shape[1], im.shape[0]))  # Resize mask to match image dimensions\n",
        "\n",
        "        detection = fo.Detection(\n",
        "            label=label,\n",
        "            bounding_box=[\n",
        "                bbox[0] / im.shape[1],\n",
        "                bbox[1] / im.shape[0],\n",
        "                (bbox[2] - bbox[0]) / im.shape[1],\n",
        "                (bbox[3] - bbox[1]) / im.shape[0]\n",
        "            ],\n",
        "            confidence=score,\n",
        "            mask=mask\n",
        "        )\n",
        "        detections.append(detection)\n",
        "\n",
        "    sample[\"predictions\"] = fo.Detections(detections=detections)\n",
        "    sample.save()\n",
        "\n",
        "\n",
        "results = dataset.evaluate_detections(\n",
        "    \"predictions\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"eval\",\n",
        "    compute_mAP=True\n",
        ")# Evaluate the predictions\n",
        "\n",
        "# Print evaluation results\n",
        "print(\"\\nDetailed Evaluation Report:\")\n",
        "print(results.report())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu8Xd_bgbjwQ",
        "outputId": "159656e9-870b-421c-c105-3a0a4b9f1533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only found 149 (<3000) samples matching your requirements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fiftyone.utils.coco:Only found 149 (<3000) samples matching your requirements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing dataset 'coco-2017-validation-3000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading existing dataset 'coco-2017-validation-3000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[09/04 14:28:24 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x/138205316/model_final_a3ec72.pkl ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "model_final_a3ec72.pkl: 254MB [00:01, 235MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating detections...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.eval.detection:Evaluating detections...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 149/149 [17.7s elapsed, 0s remaining, 7.0 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 149/149 [17.7s elapsed, 0s remaining, 7.0 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing IoU sweep...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.eval.coco:Performing IoU sweep...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 149/149 [8.1s elapsed, 0s remaining, 15.4 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 149/149 [8.1s elapsed, 0s remaining, 15.4 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Average Precision (mAP):\n",
            "0.30535134965939886\n",
            "\n",
            "Detailed Evaluation Report:\n",
            "{'airplane': {'precision': 0.6666666666666666, 'recall': 0.8, 'f1-score': 0.7272727272727272, 'support': 5}, 'apple': {'precision': 0.6666666666666666, 'recall': 0.3333333333333333, 'f1-score': 0.4444444444444444, 'support': 6}, 'backpack': {'precision': 0.6785714285714286, 'recall': 0.34545454545454546, 'f1-score': 0.45783132530120485, 'support': 55}, 'banana': {'precision': 1.0, 'recall': 0.25, 'f1-score': 0.4, 'support': 20}, 'baseball glove': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'bed': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'bench': {'precision': 0.5, 'recall': 0.3793103448275862, 'f1-score': 0.4313725490196078, 'support': 29}, 'bicycle': {'precision': 0.7448559670781894, 'recall': 0.5727848101265823, 'f1-score': 0.6475849731663686, 'support': 316}, 'bird': {'precision': 0.4, 'recall': 0.13333333333333333, 'f1-score': 0.2, 'support': 15}, 'boat': {'precision': 0.8571428571428571, 'recall': 0.75, 'f1-score': 0.7999999999999999, 'support': 32}, 'book': {'precision': 1.0, 'recall': 0.36363636363636365, 'f1-score': 0.5333333333333333, 'support': 22}, 'bottle': {'precision': 0.5, 'recall': 0.7272727272727273, 'f1-score': 0.5925925925925926, 'support': 11}, 'bowl': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'broccoli': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'bus': {'precision': 0.675, 'recall': 0.7941176470588235, 'f1-score': 0.7297297297297296, 'support': 34}, 'cake': {'precision': 1.0, 'recall': 0.42857142857142855, 'f1-score': 0.6, 'support': 14}, 'car': {'precision': 0.7660550458715596, 'recall': 0.780373831775701, 'f1-score': 0.7731481481481481, 'support': 214}, 'cat': {'precision': 1.0, 'recall': 0.6666666666666666, 'f1-score': 0.8, 'support': 3}, 'cell phone': {'precision': 0.6666666666666666, 'recall': 0.25, 'f1-score': 0.36363636363636365, 'support': 8}, 'chair': {'precision': 0.5, 'recall': 0.3235294117647059, 'f1-score': 0.3928571428571429, 'support': 68}, 'clock': {'precision': 0.8, 'recall': 0.8, 'f1-score': 0.8000000000000002, 'support': 5}, 'couch': {'precision': 0.6666666666666666, 'recall': 0.4, 'f1-score': 0.5, 'support': 5}, 'cow': {'precision': 0.6, 'recall': 0.6666666666666666, 'f1-score': 0.631578947368421, 'support': 9}, 'cup': {'precision': 0.7692307692307693, 'recall': 0.7142857142857143, 'f1-score': 0.7407407407407408, 'support': 14}, 'dining table': {'precision': 0.5714285714285714, 'recall': 0.2857142857142857, 'f1-score': 0.38095238095238093, 'support': 14}, 'dog': {'precision': 0.5, 'recall': 0.5833333333333334, 'f1-score': 0.5384615384615384, 'support': 12}, 'fire hydrant': {'precision': 0.6666666666666666, 'recall': 0.5714285714285714, 'f1-score': 0.6153846153846153, 'support': 7}, 'frisbee': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'handbag': {'precision': 0.4878048780487805, 'recall': 0.2857142857142857, 'f1-score': 0.36036036036036034, 'support': 70}, 'horse': {'precision': 0.8, 'recall': 0.6666666666666666, 'f1-score': 0.7272727272727272, 'support': 6}, 'hot dog': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 6}, 'kite': {'precision': 0.3333333333333333, 'recall': 1.0, 'f1-score': 0.5, 'support': 1}, 'knife': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'laptop': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'motorcycle': {'precision': 0.6666666666666666, 'recall': 0.6410256410256411, 'f1-score': 0.6535947712418301, 'support': 78}, 'orange': {'precision': 1.0, 'recall': 0.2, 'f1-score': 0.33333333333333337, 'support': 5}, 'parking meter': {'precision': 0.5, 'recall': 0.4, 'f1-score': 0.4444444444444445, 'support': 5}, 'person': {'precision': 0.8540772532188842, 'recall': 0.8414376321353065, 'f1-score': 0.8477103301384452, 'support': 946}, 'pizza': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'potted plant': {'precision': 0.2916666666666667, 'recall': 0.4666666666666667, 'f1-score': 0.35897435897435903, 'support': 15}, 'refrigerator': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'remote': {'precision': 0.6, 'recall': 0.75, 'f1-score': 0.6666666666666665, 'support': 4}, 'sandwich': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2}, 'scissors': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'sink': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'skateboard': {'precision': 0.8823529411764706, 'recall': 0.7142857142857143, 'f1-score': 0.7894736842105262, 'support': 21}, 'spoon': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'sports ball': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'stop sign': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}, 'suitcase': {'precision': 0.7142857142857143, 'recall': 0.5882352941176471, 'f1-score': 0.6451612903225806, 'support': 17}, 'surfboard': {'precision': 0.8888888888888888, 'recall': 0.6666666666666666, 'f1-score': 0.761904761904762, 'support': 12}, 'teddy bear': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 1}, 'tennis racket': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'tie': {'precision': 0.2, 'recall': 0.5, 'f1-score': 0.28571428571428575, 'support': 2}, 'traffic light': {'precision': 0.6451612903225806, 'recall': 0.6, 'f1-score': 0.6217616580310881, 'support': 100}, 'train': {'precision': 0.75, 'recall': 0.6666666666666666, 'f1-score': 0.7058823529411765, 'support': 9}, 'truck': {'precision': 0.64, 'recall': 0.5925925925925926, 'f1-score': 0.6153846153846153, 'support': 27}, 'tv': {'precision': 0.4, 'recall': 0.5, 'f1-score': 0.4444444444444445, 'support': 4}, 'umbrella': {'precision': 0.5263157894736842, 'recall': 0.6060606060606061, 'f1-score': 0.5633802816901409, 'support': 33}, 'vase': {'precision': 0.5, 'recall': 0.25, 'f1-score': 0.3333333333333333, 'support': 8}, 'micro avg': {'precision': 0.7529013539651838, 'recall': 0.6751951431049437, 'f1-score': 0.7119341563786009, 'support': 2306}, 'macro avg': {'precision': 0.5229361899123063, 'recall': 0.44759719079748045, 'f1-score': 0.4571064319914191, 'support': 2306}, 'weighted avg': {'precision': 0.7515660905681053, 'recall': 0.6751951431049437, 'f1-score': 0.7020289768615073, 'support': 2306}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"validation\",\n",
        "    label_types=[\"detections\"],\n",
        "    max_samples=3000,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "model = YOLO(\"yolov8l.pt\") # Load YOLOv8L model\n",
        "\n",
        "# Function to run inference on an image\n",
        "def run_inference(image_path):\n",
        "    im = cv2.imread(image_path)\n",
        "    results = model(im)\n",
        "    return results, im\n",
        "\n",
        "# Map class indices to class names\n",
        "coco_classes = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "               'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
        "               'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
        "               'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "               'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "               'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
        "               'hair drier', 'toothbrush']\n",
        "\n",
        "class_to_id = {cls: i for i, cls in enumerate(coco_classes)}\n",
        "id_to_class = {v: k for k, v in class_to_id.items()}\n",
        "\n",
        "\n",
        "for sample in dataset.iter_samples():\n",
        "    image_path = sample.filepath\n",
        "    results, im = run_inference(image_path)\n",
        "\n",
        "    # Convert YOLO outputs to FiftyOne format\n",
        "    detections = []\n",
        "    for result in results:\n",
        "        # Ensure results are in the expected format\n",
        "        if hasattr(result, 'boxes') and result.boxes is not None:\n",
        "            for bbox, score, class_id in zip(result.boxes.xyxy.tolist(), result.boxes.conf.tolist(), result.boxes.cls.tolist()):\n",
        "                bbox = bbox  # Bounding box format is already a list of floats\n",
        "                score = float(score)\n",
        "                class_id = int(class_id)\n",
        "\n",
        "                # Check if class_id is within the range of class_to_id\n",
        "                if class_id not in class_to_id.values():\n",
        "                    continue  # Skip this detection\n",
        "\n",
        "                label = id_to_class[class_id]  # Convert class_id to string label\n",
        "\n",
        "                detection = fo.Detection(\n",
        "                    label=label,\n",
        "                    bounding_box=[\n",
        "                        bbox[0] / im.shape[1],\n",
        "                        bbox[1] / im.shape[0],\n",
        "                        (bbox[2] - bbox[0]) / im.shape[1],\n",
        "                        (bbox[3] - bbox[1]) / im.shape[0]\n",
        "                    ],\n",
        "                    confidence=score\n",
        "                )\n",
        "                detections.append(detection)\n",
        "\n",
        "    sample[\"predictions\"] = fo.Detections(detections=detections)\n",
        "    sample.save()\n",
        "\n",
        "\n",
        "results = dataset.evaluate_detections(\n",
        "    \"predictions\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"eval\",\n",
        "    compute_mAP=True\n",
        ")# Evaluate the predictions\n",
        "\n",
        "print(\"\\nDetailed Evaluation Report:\")\n",
        "print(results.report())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwYyuZJic1b7",
        "outputId": "35adf090-39ed-4371-b6c5-1925168c5f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only found 149 (<3000) samples matching your requirements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fiftyone.utils.coco:Only found 149 (<3000) samples matching your requirements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing dataset 'coco-2017-validation-3000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading existing dataset 'coco-2017-validation-3000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 448x640 4 persons, 1 bicycle, 15 cars, 11 buss, 50.9ms\n",
            "Speed: 1.5ms preprocess, 50.9ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 416x640 3 persons, 1 bicycle, 10 motorcycles, 36.8ms\n",
            "Speed: 1.5ms preprocess, 36.8ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "0: 448x640 11 persons, 6 bicycles, 1 car, 1 motorcycle, 1 traffic light, 1 handbag, 1 cup, 1 teddy bear, 37.2ms\n",
            "Speed: 1.6ms preprocess, 37.2ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 21 persons, 1 bicycle, 2 cars, 3 motorcycles, 1 bus, 38.7ms\n",
            "Speed: 1.3ms preprocess, 38.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 1 person, 1 bicycle, 7 cars, 1 motorcycle, 1 parking meter, 35.4ms\n",
            "Speed: 1.3ms preprocess, 35.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 512x640 2 persons, 3 cars, 1 truck, 9 traffic lights, 37.6ms\n",
            "Speed: 2.3ms preprocess, 37.6ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "0: 448x640 23 persons, 13 bicycles, 1 handbag, 30.5ms\n",
            "Speed: 1.3ms preprocess, 30.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 4 persons, 4 bicycles, 2 cars, 1 bus, 1 truck, 15 boats, 1 bench, 29.4ms\n",
            "Speed: 1.2ms preprocess, 29.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 2 persons, 3 bicycles, 2 cars, 2 trucks, 3 traffic lights, 31.3ms\n",
            "Speed: 1.3ms preprocess, 31.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 6 persons, 2 cars, 1 bus, 30.4ms\n",
            "Speed: 1.3ms preprocess, 30.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 4 persons, 1 bicycle, 1 apple, 1 orange, 1 cell phone, 30.4ms\n",
            "Speed: 2.3ms preprocess, 30.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x448 2 persons, 1 bicycle, 1 handbag, 26.1ms\n",
            "Speed: 1.9ms preprocess, 26.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 480x640 12 persons, 1 bicycle, 1 car, 2 horses, 7 umbrellas, 1 chair, 25.5ms\n",
            "Speed: 2.4ms preprocess, 25.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 2 persons, 5 bicycles, 23.8ms\n",
            "Speed: 1.5ms preprocess, 23.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 3 persons, 1 bicycle, 2 chairs, 25.4ms\n",
            "Speed: 3.0ms preprocess, 25.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 8 persons, 1 bicycle, 2 cars, 6 traffic lights, 1 clock, 23.9ms\n",
            "Speed: 1.4ms preprocess, 23.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 448x640 14 persons, 6 skateboards, 24.8ms\n",
            "Speed: 1.4ms preprocess, 24.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 14 persons, 7 bicycles, 2 backpacks, 1 handbag, 25.1ms\n",
            "Speed: 1.5ms preprocess, 25.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 bicycles, 1 bus, 1 traffic light, 24.7ms\n",
            "Speed: 1.4ms preprocess, 24.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 4 persons, 14 cars, 1 motorcycle, 1 bus, 3 traffic lights, 25.1ms\n",
            "Speed: 2.3ms preprocess, 25.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 bicycle, 2 cats, 24.6ms\n",
            "Speed: 1.4ms preprocess, 24.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x448 2 persons, 1 bicycle, 2 dogs, 1 tv, 24.0ms\n",
            "Speed: 1.3ms preprocess, 24.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 480x640 15 persons, 1 car, 1 motorcycle, 1 dog, 1 umbrella, 7 handbags, 1 banana, 25.4ms\n",
            "Speed: 2.2ms preprocess, 25.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 8 persons, 1 handbag, 23.8ms\n",
            "Speed: 1.5ms preprocess, 23.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 3 persons, 1 bicycle, 1 skateboard, 24.0ms\n",
            "Speed: 1.5ms preprocess, 24.0ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 2 persons, 1 car, 1 truck, 25.8ms\n",
            "Speed: 1.6ms preprocess, 25.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 6 persons, 3 bicycles, 1 umbrella, 1 handbag, 1 suitcase, 23.7ms\n",
            "Speed: 1.3ms preprocess, 23.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x640 1 bench, 1 surfboard, 31.0ms\n",
            "Speed: 3.1ms preprocess, 31.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x480 3 persons, 1 bicycle, 3 cars, 2 trucks, 1 backpack, 25.5ms\n",
            "Speed: 1.1ms preprocess, 25.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 5 persons, 1 bicycle, 1 train, 1 backpack, 1 frisbee, 25.8ms\n",
            "Speed: 1.8ms preprocess, 25.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 2 persons, 1 bicycle, 1 frisbee, 1 skateboard, 25.0ms\n",
            "Speed: 1.9ms preprocess, 25.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 3 persons, 2 bicycles, 1 motorcycle, 1 handbag, 25.1ms\n",
            "Speed: 2.5ms preprocess, 25.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x448 1 person, 2 bicycles, 1 skateboard, 23.7ms\n",
            "Speed: 1.2ms preprocess, 23.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 480x640 1 bicycle, 5 cups, 1 bowl, 3 chairs, 2 potted plants, 1 dining table, 2 books, 1 clock, 1 vase, 25.6ms\n",
            "Speed: 1.7ms preprocess, 25.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x448 1 umbrella, 23.7ms\n",
            "Speed: 3.0ms preprocess, 23.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x448 1 person, 1 bicycle, 4 cars, 1 stop sign, 2 handbags, 1 cell phone, 22.9ms\n",
            "Speed: 1.4ms preprocess, 22.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 480x640 8 persons, 1 bicycle, 2 horses, 1 backpack, 25.3ms\n",
            "Speed: 1.6ms preprocess, 25.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 3 bicycles, 1 motorcycle, 2 potted plants, 24.2ms\n",
            "Speed: 1.3ms preprocess, 24.2ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x576 12 persons, 1 bicycle, 1 kite, 29.8ms\n",
            "Speed: 1.5ms preprocess, 29.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "0: 448x640 8 persons, 1 bicycle, 2 cars, 2 motorcycles, 2 traffic lights, 2 umbrellas, 2 handbags, 25.4ms\n",
            "Speed: 1.3ms preprocess, 25.4ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x480 11 persons, 1 bicycle, 1 car, 6 motorcycles, 2 trucks, 1 fire hydrant, 1 backpack, 1 umbrella, 1 suitcase, 24.3ms\n",
            "Speed: 1.4ms preprocess, 24.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 512x640 6 persons, 2 bicycles, 1 cell phone, 26.3ms\n",
            "Speed: 2.1ms preprocess, 26.3ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "0: 384x640 5 persons, 1 sports ball, 22.3ms\n",
            "Speed: 1.8ms preprocess, 22.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 448x640 4 persons, 3 bicycles, 3 cars, 1 backpack, 3 skateboards, 24.9ms\n",
            "Speed: 1.4ms preprocess, 24.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 2 bicycles, 1 car, 1 motorcycle, 1 dog, 24.1ms\n",
            "Speed: 2.5ms preprocess, 24.1ms inference, 2.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 2 bicycles, 1 cat, 25.7ms\n",
            "Speed: 1.3ms preprocess, 25.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 bicycle, 3 cars, 1 bench, 1 dog, 28.6ms\n",
            "Speed: 1.3ms preprocess, 28.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 4 persons, 3 bicycles, 1 car, 1 bus, 1 bench, 23.4ms\n",
            "Speed: 2.3ms preprocess, 23.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 2 persons, 2 motorcycles, 1 chair, 25.7ms\n",
            "Speed: 1.3ms preprocess, 25.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 352x640 1 person, 1 bicycle, 1 boat, 20.2ms\n",
            "Speed: 1.0ms preprocess, 20.2ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 512x640 2 persons, 3 bicycles, 4 cars, 1 truck, 1 dog, 1 backpack, 1 handbag, 26.9ms\n",
            "Speed: 1.4ms preprocess, 26.9ms inference, 1.7ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "0: 480x640 6 persons, 1 bicycle, 3 cars, 1 truck, 1 fire hydrant, 1 parking meter, 9 chairs, 2 potted plants, 25.5ms\n",
            "Speed: 1.3ms preprocess, 25.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x640 13 persons, 2 bicycles, 2 benchs, 31.6ms\n",
            "Speed: 2.6ms preprocess, 31.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x448 1 person, 10 cars, 2 traffic lights, 23.7ms\n",
            "Speed: 1.4ms preprocess, 23.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x448 12 persons, 1 bicycle, 3 cars, 1 bench, 3 backpacks, 22.7ms\n",
            "Speed: 1.4ms preprocess, 22.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 448x640 4 persons, 3 bicycles, 1 motorcycle, 1 umbrella, 25.1ms\n",
            "Speed: 3.3ms preprocess, 25.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 14 persons, 1 bicycle, 1 bus, 3 birds, 5 chairs, 3 potted plants, 24.4ms\n",
            "Speed: 1.5ms preprocess, 24.4ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x448 1 bicycle, 23.5ms\n",
            "Speed: 1.4ms preprocess, 23.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x448 1 person, 4 bicycles, 1 motorcycle, 22.6ms\n",
            "Speed: 1.3ms preprocess, 22.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x448 1 person, 1 skateboard, 22.5ms\n",
            "Speed: 1.2ms preprocess, 22.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 448x640 5 persons, 1 bicycle, 4 motorcycles, 1 potted plant, 25.3ms\n",
            "Speed: 1.5ms preprocess, 25.3ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 416x640 8 persons, 1 bicycle, 2 cars, 2 buss, 1 truck, 8 cows, 24.5ms\n",
            "Speed: 1.3ms preprocess, 24.5ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "0: 640x480 2 persons, 2 bicycles, 8 suitcases, 24.2ms\n",
            "Speed: 1.4ms preprocess, 24.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 8 persons, 2 bicycles, 2 dogs, 1 sheep, 5 umbrellas, 23.6ms\n",
            "Speed: 1.5ms preprocess, 23.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 1 bicycle, 1 handbag, 1 bed, 26.0ms\n",
            "Speed: 1.2ms preprocess, 26.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 2 bicycles, 1 train, 24.3ms\n",
            "Speed: 2.4ms preprocess, 24.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x544 1 person, 1 bicycle, 1 bench, 1 dog, 30.7ms\n",
            "Speed: 2.7ms preprocess, 30.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "0: 640x480 4 persons, 1 bicycle, 3 cars, 7 umbrellas, 1 handbag, 3 chairs, 2 dining tables, 23.9ms\n",
            "Speed: 1.9ms preprocess, 23.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 448x640 9 persons, 1 bicycle, 3 motorcycles, 1 bus, 24.4ms\n",
            "Speed: 1.4ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 1 bicycle, 3 cars, 1 fire hydrant, 28.6ms\n",
            "Speed: 1.3ms preprocess, 28.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 8 persons, 4 traffic lights, 1 backpack, 1 umbrella, 1 handbag, 1 potted plant, 24.8ms\n",
            "Speed: 1.4ms preprocess, 24.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 19 persons, 4 bicycles, 8 cars, 1 motorcycle, 1 truck, 4 handbags, 25.6ms\n",
            "Speed: 1.8ms preprocess, 25.6ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 544x640 1 bicycle, 1 fire hydrant, 34.6ms\n",
            "Speed: 2.0ms preprocess, 34.6ms inference, 3.5ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "0: 480x640 19 persons, 2 bicycles, 1 train, 5 backpacks, 26.0ms\n",
            "Speed: 2.0ms preprocess, 26.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 1 bicycle, 3 cars, 1 bus, 1 truck, 1 bench, 24.7ms\n",
            "Speed: 1.3ms preprocess, 24.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 13 persons, 1 bicycle, 2 skateboards, 25.5ms\n",
            "Speed: 1.3ms preprocess, 25.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 2 persons, 3 bicycles, 5 cars, 2 boats, 24.8ms\n",
            "Speed: 1.2ms preprocess, 24.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x480 2 persons, 1 bicycle, 1 book, 24.2ms\n",
            "Speed: 2.2ms preprocess, 24.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 3 persons, 1 bicycle, 1 bus, 1 traffic light, 26.0ms\n",
            "Speed: 3.3ms preprocess, 26.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 18 persons, 1 bicycle, 1 motorcycle, 3 buss, 2 trucks, 1 traffic light, 1 handbag, 2 clocks, 24.7ms\n",
            "Speed: 1.3ms preprocess, 24.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 5 persons, 1 bicycle, 8 cars, 1 motorcycle, 2 traffic lights, 25.7ms\n",
            "Speed: 1.2ms preprocess, 25.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x512 35 persons, 2 bicycles, 7 surfboards, 24.1ms\n",
            "Speed: 1.3ms preprocess, 24.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "0: 640x448 1 person, 1 bicycle, 1 skateboard, 23.1ms\n",
            "Speed: 2.1ms preprocess, 23.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x640 2 persons, 1 bicycle, 1 bus, 4 traffic lights, 32.0ms\n",
            "Speed: 1.7ms preprocess, 32.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 480x640 2 persons, 1 bicycle, 3 bananas, 4 apples, 1 orange, 25.7ms\n",
            "Speed: 1.5ms preprocess, 25.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 512x640 17 persons, 1 bicycle, 1 motorcycle, 1 train, 1 backpack, 26.5ms\n",
            "Speed: 1.4ms preprocess, 26.5ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "0: 480x640 8 persons, 4 bicycles, 4 cars, 1 bus, 6 traffic lights, 1 handbag, 26.0ms\n",
            "Speed: 1.9ms preprocess, 26.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 1 bicycle, 24.9ms\n",
            "Speed: 2.5ms preprocess, 24.9ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 9 persons, 1 bicycle, 27.4ms\n",
            "Speed: 1.8ms preprocess, 27.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 8 bicycles, 24.9ms\n",
            "Speed: 1.4ms preprocess, 24.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 6 persons, 4 bicycles, 2 trucks, 1 stop sign, 1 kite, 1 clock, 25.0ms\n",
            "Speed: 1.3ms preprocess, 25.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 5 persons, 2 bicycles, 6 cars, 3 umbrellas, 6 chairs, 1 dining table, 24.1ms\n",
            "Speed: 1.2ms preprocess, 24.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 1 person, 1 bicycle, 1 bottle, 2 spoons, 6 sandwichs, 2 cakes, 2 potted plants, 1 dining table, 25.4ms\n",
            "Speed: 1.3ms preprocess, 25.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x448 4 persons, 1 bicycle, 1 bus, 26.8ms\n",
            "Speed: 1.2ms preprocess, 26.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x480 2 bicycles, 1 clock, 24.6ms\n",
            "Speed: 1.2ms preprocess, 24.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 10 persons, 10 benchs, 2 suitcases, 1 chair, 1 dining table, 25.8ms\n",
            "Speed: 2.5ms preprocess, 25.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 416x640 1 bicycle, 7 cars, 1 truck, 1 traffic light, 24.1ms\n",
            "Speed: 1.2ms preprocess, 24.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "0: 480x640 17 persons, 3 bicycles, 2 cars, 1 backpack, 25.6ms\n",
            "Speed: 1.3ms preprocess, 25.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 bicycle, 2 chairs, 1 couch, 4 potted plants, 1 tv, 3 remotes, 24.7ms\n",
            "Speed: 1.3ms preprocess, 24.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 3 persons, 1 bicycle, 2 cars, 2 umbrellas, 1 chair, 24.6ms\n",
            "Speed: 2.7ms preprocess, 24.6ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 384x640 10 persons, 1 bicycle, 7 cars, 1 bus, 17 traffic lights, 20.0ms\n",
            "Speed: 1.8ms preprocess, 20.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 bicycles, 1 handbag, 1 apple, 25.6ms\n",
            "Speed: 3.0ms preprocess, 25.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x640 1 person, 1 bicycle, 1 train, 32.4ms\n",
            "Speed: 3.5ms preprocess, 32.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 448x640 14 persons, 1 bicycle, 4 traffic lights, 1 handbag, 2 potted plants, 26.0ms\n",
            "Speed: 1.3ms preprocess, 26.0ms inference, 3.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x448 1 person, 1 bicycle, 1 bottle, 25.2ms\n",
            "Speed: 1.4ms preprocess, 25.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 480x640 1 person, 2 benchs, 5 bottles, 2 cups, 2 sandwichs, 1 hot dog, 1 dining table, 27.6ms\n",
            "Speed: 2.2ms preprocess, 27.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x448 11 persons, 1 bicycle, 3 bananas, 25.1ms\n",
            "Speed: 1.2ms preprocess, 25.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 448x640 13 persons, 3 bicycles, 5 cars, 2 motorcycles, 27.0ms\n",
            "Speed: 1.2ms preprocess, 27.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 bicycle, 1 traffic light, 1 handbag, 25.7ms\n",
            "Speed: 1.2ms preprocess, 25.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 15 persons, 1 bicycle, 2 traffic lights, 1 backpack, 1 handbag, 26.3ms\n",
            "Speed: 2.4ms preprocess, 26.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x480 1 bicycle, 1 cup, 3 chairs, 1 tv, 1 remote, 26.5ms\n",
            "Speed: 2.3ms preprocess, 26.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 1 person, 1 motorcycle, 28.4ms\n",
            "Speed: 1.3ms preprocess, 28.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 12 persons, 1 bicycle, 2 cars, 1 traffic light, 1 backpack, 1 handbag, 6 potted plants, 27.2ms\n",
            "Speed: 1.5ms preprocess, 27.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x416 2 persons, 4 bicycles, 1 car, 2 umbrellas, 22.6ms\n",
            "Speed: 1.2ms preprocess, 22.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
            "\n",
            "0: 640x448 10 persons, 2 bicycles, 4 motorcycles, 1 parking meter, 22.1ms\n",
            "Speed: 1.5ms preprocess, 22.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 448x640 3 persons, 1 bicycle, 1 car, 4 buss, 2 trains, 1 truck, 1 stop sign, 24.7ms\n",
            "Speed: 3.7ms preprocess, 24.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 544x640 1 bicycle, 30.1ms\n",
            "Speed: 3.8ms preprocess, 30.1ms inference, 1.6ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "0: 448x640 9 persons, 6 bicycles, 1 backpack, 1 handbag, 1 suitcase, 24.7ms\n",
            "Speed: 1.2ms preprocess, 24.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 384x640 2 bicycles, 1 car, 1 cat, 21.6ms\n",
            "Speed: 1.7ms preprocess, 21.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 448x640 1 bicycle, 2 cars, 23.8ms\n",
            "Speed: 2.3ms preprocess, 23.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x448 3 persons, 1 bicycle, 1 bottle, 2 cups, 23.0ms\n",
            "Speed: 1.2ms preprocess, 23.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 480x640 1 bicycle, 1 airplane, 1 bottle, 25.4ms\n",
            "Speed: 1.3ms preprocess, 25.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 4 persons, 1 bicycle, 4 cars, 5 traffic lights, 2 backpacks, 25.6ms\n",
            "Speed: 1.8ms preprocess, 25.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 512x640 1 person, 1 bicycle, 1 tie, 25.6ms\n",
            "Speed: 1.3ms preprocess, 25.6ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "0: 480x640 1 person, 1 bicycle, 1 parking meter, 26.7ms\n",
            "Speed: 2.4ms preprocess, 26.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 bicycles, 1 train, 5 benchs, 1 backpack, 1 dining table, 1 laptop, 24.5ms\n",
            "Speed: 1.3ms preprocess, 24.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 10 persons, 2 bicycles, 1 car, 4 motorcycles, 3 traffic lights, 1 fire hydrant, 1 backpack, 1 handbag, 24.3ms\n",
            "Speed: 1.6ms preprocess, 24.3ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 512x640 1 bicycle, 10 cars, 8 boats, 25.4ms\n",
            "Speed: 1.4ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "0: 480x640 20 persons, 1 bicycle, 24.9ms\n",
            "Speed: 1.5ms preprocess, 24.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 9 persons, 9 bicycles, 1 motorcycle, 1 dog, 2 handbags, 1 skateboard, 1 cup, 24.4ms\n",
            "Speed: 1.5ms preprocess, 24.4ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 7 persons, 1 bicycle, 1 handbag, 1 skateboard, 22.9ms\n",
            "Speed: 2.8ms preprocess, 22.9ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 2 persons, 1 bicycle, 1 couch, 1 dining table, 2 vases, 25.4ms\n",
            "Speed: 1.2ms preprocess, 25.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x448 1 person, 2 bicycles, 23.9ms\n",
            "Speed: 1.8ms preprocess, 23.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x448 2 bicycles, 1 car, 23.2ms\n",
            "Speed: 1.2ms preprocess, 23.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 448x640 1 person, 1 bicycle, 2 horses, 1 backpack, 24.4ms\n",
            "Speed: 1.2ms preprocess, 24.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 2 bicycles, 2 motorcycles, 24.4ms\n",
            "Speed: 3.2ms preprocess, 24.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 544x640 13 persons, 1 bicycle, 4 cars, 1 traffic light, 1 backpack, 1 handbag, 28.2ms\n",
            "Speed: 1.4ms preprocess, 28.2ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "0: 640x448 3 persons, 1 bicycle, 24.4ms\n",
            "Speed: 1.2ms preprocess, 24.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x448 4 persons, 1 bicycle, 7 cars, 5 traffic lights, 23.2ms\n",
            "Speed: 2.8ms preprocess, 23.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 480x640 13 persons, 3 bicycles, 1 motorcycle, 1 handbag, 26.7ms\n",
            "Speed: 1.2ms preprocess, 26.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 2 persons, 1 train, 2 skateboards, 25.4ms\n",
            "Speed: 1.2ms preprocess, 25.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 2 persons, 2 airplanes, 3 bottles, 2 chairs, 24.4ms\n",
            "Speed: 1.2ms preprocess, 24.4ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 12 persons, 1 bicycle, 2 cars, 1 train, 23.6ms\n",
            "Speed: 1.2ms preprocess, 23.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 3 persons, 2 bicycles, 2 cars, 1 backpack, 24.8ms\n",
            "Speed: 1.2ms preprocess, 24.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 384x640 5 persons, 1 bicycle, 8 benchs, 20.0ms\n",
            "Speed: 1.5ms preprocess, 20.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 480x640 13 persons, 8 bicycles, 4 cars, 19 motorcycles, 1 bus, 2 trucks, 1 traffic light, 1 umbrella, 24.3ms\n",
            "Speed: 1.5ms preprocess, 24.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 15 persons, 2 bicycles, 1 backpack, 1 frisbee, 4 surfboards, 24.0ms\n",
            "Speed: 1.2ms preprocess, 24.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 bicycles, 2 cars, 1 skateboard, 25.2ms\n",
            "Speed: 1.7ms preprocess, 25.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 13 persons, 1 bicycle, 3 motorcycles, 1 backpack, 24.4ms\n",
            "Speed: 2.9ms preprocess, 24.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Evaluating detections...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.eval.detection:Evaluating detections...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 149/149 [11.5s elapsed, 0s remaining, 16.2 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 149/149 [11.5s elapsed, 0s remaining, 16.2 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing IoU sweep...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.eval.coco:Performing IoU sweep...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 149/149 [5.5s elapsed, 0s remaining, 35.5 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 149/149 [5.5s elapsed, 0s remaining, 35.5 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Average Precision (mAP):\n",
            "0.3540234344650885\n",
            "\n",
            "Detailed Evaluation Report:\n",
            "{'airplane': {'precision': 1.0, 'recall': 0.6, 'f1-score': 0.7499999999999999, 'support': 5}, 'apple': {'precision': 0.3333333333333333, 'recall': 0.3333333333333333, 'f1-score': 0.3333333333333333, 'support': 6}, 'backpack': {'precision': 0.5483870967741935, 'recall': 0.3090909090909091, 'f1-score': 0.39534883720930236, 'support': 55}, 'banana': {'precision': 1.0, 'recall': 0.3684210526315789, 'f1-score': 0.5384615384615384, 'support': 19}, 'bed': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'bench': {'precision': 0.5, 'recall': 0.5862068965517241, 'f1-score': 0.5396825396825397, 'support': 29}, 'bicycle': {'precision': 0.7751004016064257, 'recall': 0.6107594936708861, 'f1-score': 0.6831858407079646, 'support': 316}, 'bird': {'precision': 1.0, 'recall': 0.2, 'f1-score': 0.33333333333333337, 'support': 15}, 'boat': {'precision': 0.9230769230769231, 'recall': 0.7272727272727273, 'f1-score': 0.8135593220338984, 'support': 33}, 'book': {'precision': 1.0, 'recall': 0.13636363636363635, 'f1-score': 0.24000000000000002, 'support': 22}, 'bottle': {'precision': 0.5833333333333334, 'recall': 0.6363636363636364, 'f1-score': 0.6086956521739131, 'support': 11}, 'bowl': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'bus': {'precision': 0.7714285714285715, 'recall': 0.7941176470588235, 'f1-score': 0.782608695652174, 'support': 34}, 'cake': {'precision': 1.0, 'recall': 0.15384615384615385, 'f1-score': 0.2666666666666667, 'support': 13}, 'car': {'precision': 0.82, 'recall': 0.784688995215311, 'f1-score': 0.8019559902200488, 'support': 209}, 'cat': {'precision': 0.75, 'recall': 1.0, 'f1-score': 0.8571428571428571, 'support': 3}, 'cell phone': {'precision': 1.0, 'recall': 0.375, 'f1-score': 0.5454545454545454, 'support': 8}, 'chair': {'precision': 0.5384615384615384, 'recall': 0.31343283582089554, 'f1-score': 0.3962264150943396, 'support': 67}, 'clock': {'precision': 0.8333333333333334, 'recall': 1.0, 'f1-score': 0.9090909090909091, 'support': 5}, 'couch': {'precision': 0.5, 'recall': 0.2, 'f1-score': 0.28571428571428575, 'support': 5}, 'cow': {'precision': 0.75, 'recall': 0.6666666666666666, 'f1-score': 0.7058823529411765, 'support': 9}, 'cup': {'precision': 0.9166666666666666, 'recall': 0.7857142857142857, 'f1-score': 0.8461538461538461, 'support': 14}, 'dining table': {'precision': 0.2222222222222222, 'recall': 0.14285714285714285, 'f1-score': 0.17391304347826086, 'support': 14}, 'dog': {'precision': 0.8, 'recall': 0.6666666666666666, 'f1-score': 0.7272727272727272, 'support': 12}, 'fire hydrant': {'precision': 1.0, 'recall': 0.7142857142857143, 'f1-score': 0.8333333333333333, 'support': 7}, 'frisbee': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'handbag': {'precision': 0.45, 'recall': 0.2571428571428571, 'f1-score': 0.32727272727272727, 'support': 70}, 'horse': {'precision': 0.8333333333333334, 'recall': 0.8333333333333334, 'f1-score': 0.8333333333333334, 'support': 6}, 'hot dog': {'precision': 1.0, 'recall': 0.16666666666666666, 'f1-score': 0.2857142857142857, 'support': 6}, 'kite': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 1}, 'knife': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'laptop': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'motorcycle': {'precision': 0.6375, 'recall': 0.6455696202531646, 'f1-score': 0.6415094339622641, 'support': 79}, 'orange': {'precision': 0.5, 'recall': 0.2, 'f1-score': 0.28571428571428575, 'support': 5}, 'parking meter': {'precision': 0.75, 'recall': 0.6, 'f1-score': 0.6666666666666665, 'support': 5}, 'person': {'precision': 0.894927536231884, 'recall': 0.8178807947019867, 'f1-score': 0.8546712802768165, 'support': 906}, 'pizza': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'potted plant': {'precision': 0.28, 'recall': 0.4666666666666667, 'f1-score': 0.35000000000000003, 'support': 15}, 'refrigerator': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'remote': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 4}, 'sandwich': {'precision': 0.125, 'recall': 0.5, 'f1-score': 0.2, 'support': 2}, 'scissors': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'sheep': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'skateboard': {'precision': 0.8571428571428571, 'recall': 0.8571428571428571, 'f1-score': 0.8571428571428571, 'support': 21}, 'spoon': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'sports ball': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'stop sign': {'precision': 0.3333333333333333, 'recall': 0.25, 'f1-score': 0.28571428571428575, 'support': 4}, 'suitcase': {'precision': 0.6153846153846154, 'recall': 0.47058823529411764, 'f1-score': 0.5333333333333333, 'support': 17}, 'surfboard': {'precision': 0.8333333333333334, 'recall': 0.8333333333333334, 'f1-score': 0.8333333333333334, 'support': 12}, 'teddy bear': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'tie': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'traffic light': {'precision': 0.6744186046511628, 'recall': 0.5858585858585859, 'f1-score': 0.627027027027027, 'support': 99}, 'train': {'precision': 0.9, 'recall': 1.0, 'f1-score': 0.9473684210526316, 'support': 9}, 'truck': {'precision': 0.7272727272727273, 'recall': 0.5925925925925926, 'f1-score': 0.6530612244897959, 'support': 27}, 'tv': {'precision': 0.6666666666666666, 'recall': 0.5, 'f1-score': 0.5714285714285715, 'support': 4}, 'umbrella': {'precision': 0.7428571428571429, 'recall': 0.7878787878787878, 'f1-score': 0.7647058823529412, 'support': 33}, 'vase': {'precision': 0.6666666666666666, 'recall': 0.25, 'f1-score': 0.36363636363636365, 'support': 8}, 'micro avg': {'precision': 0.7879730430274754, 'recall': 0.6728640991589199, 'f1-score': 0.725883476599809, 'support': 2259}, 'macro avg': {'precision': 0.6325119339843905, 'recall': 0.5038551249872814, 'f1-score': 0.5277540826479973, 'support': 2259}, 'weighted avg': {'precision': 0.7888657099633001, 'recall': 0.6728640991589199, 'f1-score': 0.7150692639785027, 'support': 2259}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load 10 samples from COCO dataset with 2 specific classes\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"validation\",\n",
        "    label_types=[\"detections\"],\n",
        "    max_samples=3000,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "model = YOLO(\"yolov8x.pt\")  # # Load YOLOv8X model\n",
        "\n",
        "# Function to run inference on an image\n",
        "def run_inference(image_path):\n",
        "    im = cv2.imread(image_path)\n",
        "    results = model(im)\n",
        "    return results, im\n",
        "\n",
        "# Map class indices to class names\n",
        "coco_classes = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
        "               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
        "               'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
        "               'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
        "               'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
        "               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
        "               'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
        "               'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
        "               'hair drier', 'toothbrush']\n",
        "\n",
        "class_to_id = {cls: i for i, cls in enumerate(coco_classes)}\n",
        "id_to_class = {v: k for k, v in class_to_id.items()}\n",
        "\n",
        "\n",
        "for sample in dataset.iter_samples():\n",
        "    image_path = sample.filepath\n",
        "    results, im = run_inference(image_path)\n",
        "\n",
        "    # Convert YOLO outputs to FiftyOne format\n",
        "    detections = []\n",
        "    for result in results:\n",
        "        # Ensure results are in the expected format\n",
        "        if hasattr(result, 'boxes') and result.boxes is not None:\n",
        "            for bbox, score, class_id in zip(result.boxes.xyxy.tolist(), result.boxes.conf.tolist(), result.boxes.cls.tolist()):\n",
        "                bbox = bbox  # Bounding box format is already a list of floats\n",
        "                score = float(score)\n",
        "                class_id = int(class_id)\n",
        "\n",
        "                # Check if class_id is within the range of class_to_id\n",
        "                if class_id not in class_to_id.values():\n",
        "                    continue  # Skip this detection\n",
        "\n",
        "                label = id_to_class[class_id]  # Convert class_id to string label\n",
        "\n",
        "                detection = fo.Detection(\n",
        "                    label=label,\n",
        "                    bounding_box=[\n",
        "                        bbox[0] / im.shape[1],\n",
        "                        bbox[1] / im.shape[0],\n",
        "                        (bbox[2] - bbox[0]) / im.shape[1],\n",
        "                        (bbox[3] - bbox[1]) / im.shape[0]\n",
        "                    ],\n",
        "                    confidence=score\n",
        "                )\n",
        "                detections.append(detection)\n",
        "\n",
        "    sample[\"predictions\"] = fo.Detections(detections=detections)\n",
        "    sample.save()\n",
        "\n",
        "\n",
        "results = dataset.evaluate_detections(\n",
        "    \"predictions\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"eval\",\n",
        "    compute_mAP=True\n",
        ")# Evaluate the predictions\n",
        "\n",
        "\n",
        "print(\"\\nDetailed Evaluation Report:\")\n",
        "print(results.report())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOp5mowceq9u",
        "outputId": "21678888-ac1c-43f1-b8c3-f6b3561ab2aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Only found 149 (<3000) samples matching your requirements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fiftyone.utils.coco:Only found 149 (<3000) samples matching your requirements\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing dataset 'coco-2017-validation-3000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading existing dataset 'coco-2017-validation-3000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 448x640 7 persons, 2 bicycles, 15 cars, 12 buss, 56.5ms\n",
            "Speed: 1.7ms preprocess, 56.5ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 416x640 2 persons, 1 bicycle, 7 motorcycles, 49.4ms\n",
            "Speed: 1.4ms preprocess, 49.4ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "0: 448x640 9 persons, 7 bicycles, 1 car, 1 motorcycle, 1 backpack, 1 handbag, 1 cup, 1 teddy bear, 45.6ms\n",
            "Speed: 1.4ms preprocess, 45.6ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 21 persons, 1 bicycle, 1 car, 5 motorcycles, 1 bus, 1 truck, 45.3ms\n",
            "Speed: 1.7ms preprocess, 45.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 1 person, 4 cars, 2 motorcycles, 1 stop sign, 1 parking meter, 44.8ms\n",
            "Speed: 2.5ms preprocess, 44.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 512x640 2 persons, 4 cars, 1 truck, 7 traffic lights, 45.6ms\n",
            "Speed: 2.4ms preprocess, 45.6ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "0: 448x640 24 persons, 11 bicycles, 1 handbag, 40.4ms\n",
            "Speed: 2.1ms preprocess, 40.4ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 4 persons, 4 bicycles, 2 cars, 1 bus, 1 truck, 12 boats, 2 benchs, 39.0ms\n",
            "Speed: 1.6ms preprocess, 39.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 2 persons, 3 bicycles, 3 cars, 2 trucks, 4 traffic lights, 41.4ms\n",
            "Speed: 2.2ms preprocess, 41.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 6 persons, 3 cars, 1 bus, 40.4ms\n",
            "Speed: 1.2ms preprocess, 40.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 4 persons, 1 bicycle, 2 apples, 1 cell phone, 40.1ms\n",
            "Speed: 2.6ms preprocess, 40.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x448 2 persons, 1 bicycle, 1 handbag, 37.6ms\n",
            "Speed: 1.2ms preprocess, 37.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 480x640 11 persons, 1 car, 2 horses, 6 umbrellas, 1 chair, 36.9ms\n",
            "Speed: 1.3ms preprocess, 36.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 2 persons, 5 bicycles, 38.2ms\n",
            "Speed: 1.3ms preprocess, 38.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 3 persons, 1 bottle, 2 chairs, 39.2ms\n",
            "Speed: 2.4ms preprocess, 39.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 8 persons, 3 cars, 5 traffic lights, 1 clock, 40.4ms\n",
            "Speed: 1.2ms preprocess, 40.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 448x640 16 persons, 2 bicycles, 5 skateboards, 1 tv, 35.0ms\n",
            "Speed: 1.2ms preprocess, 35.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 14 persons, 7 bicycles, 2 backpacks, 2 handbags, 35.6ms\n",
            "Speed: 1.3ms preprocess, 35.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 bicycles, 1 bus, 36.1ms\n",
            "Speed: 1.5ms preprocess, 36.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 3 persons, 15 cars, 1 motorcycle, 1 bus, 3 traffic lights, 36.2ms\n",
            "Speed: 2.4ms preprocess, 36.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 bicycles, 1 cat, 35.7ms\n",
            "Speed: 1.3ms preprocess, 35.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x448 1 person, 2 bicycles, 3 dogs, 38.6ms\n",
            "Speed: 1.2ms preprocess, 38.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 480x640 16 persons, 2 bicycles, 1 car, 1 motorcycle, 5 handbags, 1 banana, 37.5ms\n",
            "Speed: 1.2ms preprocess, 37.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 8 persons, 1 truck, 1 handbag, 35.5ms\n",
            "Speed: 1.3ms preprocess, 35.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 3 persons, 1 bicycle, 1 skateboard, 37.4ms\n",
            "Speed: 6.3ms preprocess, 37.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 2 persons, 1 car, 1 truck, 38.1ms\n",
            "Speed: 1.2ms preprocess, 38.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 6 persons, 5 bicycles, 1 umbrella, 1 handbag, 37.7ms\n",
            "Speed: 1.3ms preprocess, 37.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x640 1 bench, 1 surfboard, 52.8ms\n",
            "Speed: 3.3ms preprocess, 52.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x480 3 persons, 1 bicycle, 5 cars, 2 trucks, 39.1ms\n",
            "Speed: 1.2ms preprocess, 39.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 4 persons, 1 bicycle, 1 train, 1 backpack, 39.0ms\n",
            "Speed: 1.3ms preprocess, 39.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 2 persons, 2 bicycles, 1 skateboard, 38.9ms\n",
            "Speed: 1.3ms preprocess, 38.9ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 2 persons, 2 bicycles, 1 motorcycle, 1 backpack, 1 handbag, 38.3ms\n",
            "Speed: 4.2ms preprocess, 38.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x448 1 person, 3 bicycles, 1 skateboard, 36.6ms\n",
            "Speed: 1.3ms preprocess, 36.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 480x640 1 bicycle, 5 cups, 1 bowl, 3 chairs, 1 potted plant, 1 dining table, 4 books, 1 vase, 38.4ms\n",
            "Speed: 1.3ms preprocess, 38.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x448 1 umbrella, 37.3ms\n",
            "Speed: 2.2ms preprocess, 37.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x448 1 person, 1 bicycle, 4 cars, 1 stop sign, 1 handbag, 1 cell phone, 37.0ms\n",
            "Speed: 3.7ms preprocess, 37.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 480x640 12 persons, 1 bicycle, 1 motorcycle, 1 dog, 2 horses, 38.4ms\n",
            "Speed: 1.3ms preprocess, 38.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 1 motorcycle, 36.7ms\n",
            "Speed: 2.7ms preprocess, 36.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x576 8 persons, 2 kites, 48.4ms\n",
            "Speed: 1.6ms preprocess, 48.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 576)\n",
            "\n",
            "0: 448x640 8 persons, 1 bicycle, 2 cars, 2 traffic lights, 2 umbrellas, 2 handbags, 37.7ms\n",
            "Speed: 2.7ms preprocess, 37.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x480 13 persons, 1 bicycle, 5 motorcycles, 2 trucks, 1 fire hydrant, 1 suitcase, 39.8ms\n",
            "Speed: 1.4ms preprocess, 39.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 512x640 6 persons, 1 bicycle, 1 boat, 39.0ms\n",
            "Speed: 1.6ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 bicycle, 1 sports ball, 32.4ms\n",
            "Speed: 1.3ms preprocess, 32.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 448x640 3 persons, 5 bicycles, 2 cars, 2 backpacks, 4 skateboards, 35.3ms\n",
            "Speed: 1.2ms preprocess, 35.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 2 bicycles, 1 car, 1 motorcycle, 1 truck, 1 dog, 35.6ms\n",
            "Speed: 2.3ms preprocess, 35.6ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 2 bicycles, 1 cat, 40.6ms\n",
            "Speed: 1.6ms preprocess, 40.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 bicycle, 4 cars, 1 dog, 39.4ms\n",
            "Speed: 1.2ms preprocess, 39.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 4 persons, 2 bicycles, 1 car, 2 benchs, 39.3ms\n",
            "Speed: 2.0ms preprocess, 39.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 2 persons, 2 motorcycles, 2 chairs, 37.4ms\n",
            "Speed: 1.4ms preprocess, 37.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 352x640 1 person, 1 bicycle, 2 boats, 33.6ms\n",
            "Speed: 1.2ms preprocess, 33.6ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "0: 512x640 3 persons, 4 bicycles, 4 cars, 1 truck, 1 dog, 1 backpack, 1 handbag, 37.2ms\n",
            "Speed: 1.5ms preprocess, 37.2ms inference, 1.8ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "0: 480x640 6 persons, 1 bicycle, 4 cars, 1 fire hydrant, 1 parking meter, 8 chairs, 1 potted plant, 1 dining table, 36.5ms\n",
            "Speed: 1.5ms preprocess, 36.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x640 14 persons, 2 bicycles, 2 benchs, 1 surfboard, 51.0ms\n",
            "Speed: 5.3ms preprocess, 51.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 640x448 1 person, 1 bicycle, 12 cars, 2 traffic lights, 39.6ms\n",
            "Speed: 1.4ms preprocess, 39.6ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x448 13 persons, 1 bicycle, 4 cars, 1 motorcycle, 2 benchs, 3 backpacks, 1 handbag, 1 potted plant, 37.2ms\n",
            "Speed: 3.3ms preprocess, 37.2ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 448x640 4 persons, 3 bicycles, 1 umbrella, 38.1ms\n",
            "Speed: 7.4ms preprocess, 38.1ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 17 persons, 1 bicycle, 2 buss, 3 birds, 9 chairs, 3 potted plants, 36.7ms\n",
            "Speed: 3.3ms preprocess, 36.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x448 1 bicycle, 37.7ms\n",
            "Speed: 1.3ms preprocess, 37.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x448 1 person, 3 bicycles, 1 motorcycle, 37.3ms\n",
            "Speed: 1.2ms preprocess, 37.3ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x448 1 person, 1 skateboard, 36.1ms\n",
            "Speed: 1.4ms preprocess, 36.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 448x640 5 persons, 1 bicycle, 4 motorcycles, 2 potted plants, 39.4ms\n",
            "Speed: 1.2ms preprocess, 39.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 416x640 7 persons, 1 bicycle, 2 cars, 1 bus, 1 truck, 8 cows, 34.4ms\n",
            "Speed: 1.1ms preprocess, 34.4ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "0: 640x480 2 persons, 9 suitcases, 36.9ms\n",
            "Speed: 1.3ms preprocess, 36.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 9 persons, 3 bicycles, 2 dogs, 1 sheep, 4 umbrellas, 34.7ms\n",
            "Speed: 1.2ms preprocess, 34.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 1 bicycle, 1 chair, 36.5ms\n",
            "Speed: 1.2ms preprocess, 36.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x480 1 bicycle, 1 train, 40.9ms\n",
            "Speed: 2.6ms preprocess, 40.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x544 1 person, 1 bicycle, 1 bench, 1 dog, 1 potted plant, 49.2ms\n",
            "Speed: 1.5ms preprocess, 49.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "0: 640x480 4 persons, 1 bicycle, 2 cars, 7 umbrellas, 1 handbag, 1 chair, 2 dining tables, 36.3ms\n",
            "Speed: 1.4ms preprocess, 36.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 448x640 8 persons, 2 bicycles, 1 bus, 35.7ms\n",
            "Speed: 1.3ms preprocess, 35.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 1 bicycle, 2 cars, 2 fire hydrants, 36.5ms\n",
            "Speed: 1.2ms preprocess, 36.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 8 persons, 1 bicycle, 4 traffic lights, 2 umbrellas, 2 handbags, 35.7ms\n",
            "Speed: 1.3ms preprocess, 35.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 20 persons, 4 bicycles, 8 cars, 1 motorcycle, 1 truck, 5 handbags, 34.3ms\n",
            "Speed: 1.3ms preprocess, 34.3ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 544x640 1 bicycle, 1 fire hydrant, 48.0ms\n",
            "Speed: 1.4ms preprocess, 48.0ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "0: 480x640 16 persons, 1 bicycle, 1 train, 6 backpacks, 2 handbags, 38.4ms\n",
            "Speed: 1.4ms preprocess, 38.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 1 bicycle, 3 cars, 1 bus, 1 truck, 1 traffic light, 1 bench, 37.6ms\n",
            "Speed: 1.3ms preprocess, 37.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 12 persons, 1 bicycle, 2 skateboards, 39.2ms\n",
            "Speed: 1.3ms preprocess, 39.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 2 persons, 9 bicycles, 5 cars, 2 boats, 2 clocks, 37.1ms\n",
            "Speed: 1.3ms preprocess, 37.1ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x480 2 persons, 1 bicycle, 1 book, 38.5ms\n",
            "Speed: 2.6ms preprocess, 38.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 2 persons, 2 bicycles, 1 bus, 2 traffic lights, 37.8ms\n",
            "Speed: 1.3ms preprocess, 37.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 21 persons, 1 bicycle, 1 motorcycle, 4 buss, 2 trucks, 1 traffic light, 1 backpack, 1 handbag, 2 clocks, 36.1ms\n",
            "Speed: 1.2ms preprocess, 36.1ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 6 persons, 1 bicycle, 8 cars, 1 motorcycle, 3 traffic lights, 37.9ms\n",
            "Speed: 1.8ms preprocess, 37.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x512 25 persons, 2 bicycles, 6 surfboards, 38.1ms\n",
            "Speed: 1.8ms preprocess, 38.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
            "\n",
            "0: 640x448 1 person, 1 bicycle, 1 skateboard, 36.2ms\n",
            "Speed: 2.4ms preprocess, 36.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x640 3 persons, 1 bicycle, 2 traffic lights, 53.1ms\n",
            "Speed: 1.7ms preprocess, 53.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 480x640 3 persons, 1 bicycle, 4 bananas, 4 apples, 1 orange, 1 dining table, 38.4ms\n",
            "Speed: 1.3ms preprocess, 38.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 512x640 17 persons, 1 bicycle, 1 motorcycle, 1 train, 1 backpack, 37.9ms\n",
            "Speed: 1.5ms preprocess, 37.9ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "0: 480x640 8 persons, 6 bicycles, 4 cars, 1 bus, 6 traffic lights, 1 handbag, 35.6ms\n",
            "Speed: 1.2ms preprocess, 35.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 1 bicycle, 35.5ms\n",
            "Speed: 2.8ms preprocess, 35.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 8 persons, 1 bicycle, 1 dog, 38.9ms\n",
            "Speed: 1.4ms preprocess, 38.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 8 bicycles, 36.0ms\n",
            "Speed: 1.3ms preprocess, 36.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 6 persons, 4 bicycles, 2 trucks, 1 stop sign, 38.4ms\n",
            "Speed: 2.8ms preprocess, 38.4ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 4 persons, 2 bicycles, 5 cars, 3 umbrellas, 6 chairs, 2 dining tables, 38.0ms\n",
            "Speed: 1.3ms preprocess, 38.0ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 1 person, 1 bicycle, 1 bottle, 1 spoon, 2 sandwichs, 1 cake, 1 microwave, 37.9ms\n",
            "Speed: 1.4ms preprocess, 37.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x448 4 persons, 1 bicycle, 1 car, 1 bus, 37.1ms\n",
            "Speed: 1.3ms preprocess, 37.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x480 2 bicycles, 1 clock, 39.5ms\n",
            "Speed: 1.4ms preprocess, 39.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 10 persons, 9 benchs, 1 suitcase, 1 chair, 1 dining table, 36.0ms\n",
            "Speed: 1.3ms preprocess, 36.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 416x640 1 bicycle, 9 cars, 1 truck, 1 traffic light, 35.4ms\n",
            "Speed: 1.3ms preprocess, 35.4ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "0: 480x640 14 persons, 3 bicycles, 2 cars, 1 handbag, 36.3ms\n",
            "Speed: 1.2ms preprocess, 36.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 bicycle, 2 chairs, 2 couchs, 4 potted plants, 3 tvs, 2 remotes, 1 book, 1 vase, 36.8ms\n",
            "Speed: 1.2ms preprocess, 36.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 3 persons, 1 bicycle, 2 cars, 2 umbrellas, 2 chairs, 41.1ms\n",
            "Speed: 2.5ms preprocess, 41.1ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 384x640 11 persons, 1 bicycle, 7 cars, 1 bus, 15 traffic lights, 32.6ms\n",
            "Speed: 1.1ms preprocess, 32.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 448x640 8 persons, 4 bicycles, 1 backpack, 1 handbag, 1 apple, 1 clock, 39.3ms\n",
            "Speed: 1.3ms preprocess, 39.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x640 1 person, 1 bicycle, 1 train, 59.4ms\n",
            "Speed: 3.3ms preprocess, 59.4ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 448x640 13 persons, 1 bicycle, 4 traffic lights, 1 backpack, 1 handbag, 2 potted plants, 37.8ms\n",
            "Speed: 1.2ms preprocess, 37.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x448 1 person, 1 bicycle, 1 bottle, 37.5ms\n",
            "Speed: 1.3ms preprocess, 37.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 480x640 2 persons, 4 bottles, 2 cups, 3 sandwichs, 4 hot dogs, 1 dining table, 40.8ms\n",
            "Speed: 1.5ms preprocess, 40.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x448 11 persons, 1 bicycle, 3 bananas, 1 broccoli, 39.5ms\n",
            "Speed: 1.4ms preprocess, 39.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 448x640 15 persons, 1 bicycle, 5 cars, 2 motorcycles, 38.1ms\n",
            "Speed: 1.3ms preprocess, 38.1ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 2 persons, 3 bicycles, 1 car, 2 traffic lights, 1 handbag, 37.8ms\n",
            "Speed: 1.3ms preprocess, 37.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 17 persons, 1 bicycle, 1 traffic light, 1 handbag, 40.9ms\n",
            "Speed: 2.4ms preprocess, 40.9ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x480 1 bicycle, 1 cat, 1 cup, 2 chairs, 1 tv, 41.0ms\n",
            "Speed: 2.4ms preprocess, 41.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 480x640 1 person, 1 bicycle, 40.3ms\n",
            "Speed: 1.3ms preprocess, 40.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 12 persons, 1 bicycle, 3 cars, 1 traffic light, 4 potted plants, 40.5ms\n",
            "Speed: 1.3ms preprocess, 40.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x416 2 persons, 5 bicycles, 1 car, 2 umbrellas, 36.5ms\n",
            "Speed: 1.3ms preprocess, 36.5ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 416)\n",
            "\n",
            "0: 640x448 11 persons, 1 bicycle, 3 motorcycles, 1 parking meter, 37.1ms\n",
            "Speed: 1.4ms preprocess, 37.1ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 448x640 3 persons, 1 bicycle, 1 car, 3 buss, 4 trains, 1 truck, 36.3ms\n",
            "Speed: 1.5ms preprocess, 36.3ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 544x640 1 bicycle, 47.0ms\n",
            "Speed: 4.7ms preprocess, 47.0ms inference, 1.7ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "0: 448x640 11 persons, 6 bicycles, 1 backpack, 37.2ms\n",
            "Speed: 1.3ms preprocess, 37.2ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 384x640 2 bicycles, 1 car, 1 cat, 34.3ms\n",
            "Speed: 1.2ms preprocess, 34.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 448x640 1 bicycle, 2 cars, 38.6ms\n",
            "Speed: 1.4ms preprocess, 38.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 640x448 3 persons, 1 bicycle, 2 handbags, 2 cups, 1 remote, 1 teddy bear, 39.4ms\n",
            "Speed: 2.3ms preprocess, 39.4ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 480x640 1 bicycle, 1 airplane, 1 bottle, 40.8ms\n",
            "Speed: 1.4ms preprocess, 40.8ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 4 persons, 1 bicycle, 4 cars, 5 traffic lights, 2 backpacks, 36.8ms\n",
            "Speed: 1.4ms preprocess, 36.8ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 512x640 1 person, 1 bicycle, 1 tie, 39.0ms\n",
            "Speed: 1.5ms preprocess, 39.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "0: 480x640 1 person, 1 bicycle, 1 parking meter, 40.8ms\n",
            "Speed: 2.4ms preprocess, 40.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 bicycles, 1 train, 3 benchs, 1 backpack, 1 laptop, 38.2ms\n",
            "Speed: 1.3ms preprocess, 38.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 10 persons, 1 bicycle, 1 car, 5 motorcycles, 3 traffic lights, 1 fire hydrant, 1 backpack, 1 handbag, 38.5ms\n",
            "Speed: 2.2ms preprocess, 38.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 512x640 2 bicycles, 10 cars, 10 boats, 37.8ms\n",
            "Speed: 1.5ms preprocess, 37.8ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "0: 480x640 19 persons, 1 bicycle, 1 tie, 1 scissors, 37.3ms\n",
            "Speed: 1.4ms preprocess, 37.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 8 persons, 10 bicycles, 1 motorcycle, 2 dogs, 2 handbags, 2 skateboards, 34.5ms\n",
            "Speed: 1.3ms preprocess, 34.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 9 persons, 1 bicycle, 1 bench, 1 backpack, 1 skateboard, 34.9ms\n",
            "Speed: 2.3ms preprocess, 34.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 bicycles, 1 dining table, 2 vases, 38.8ms\n",
            "Speed: 1.5ms preprocess, 38.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 640x448 1 person, 2 bicycles, 38.0ms\n",
            "Speed: 1.3ms preprocess, 38.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x448 4 bicycles, 1 car, 36.8ms\n",
            "Speed: 1.4ms preprocess, 36.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 448x640 1 person, 1 bicycle, 2 horses, 1 backpack, 1 bottle, 35.7ms\n",
            "Speed: 1.8ms preprocess, 35.7ms inference, 2.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 1 person, 1 bicycle, 2 motorcycles, 1 knife, 36.5ms\n",
            "Speed: 1.2ms preprocess, 36.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 544x640 12 persons, 1 bicycle, 4 cars, 2 traffic lights, 1 backpack, 2 umbrellas, 1 handbag, 46.8ms\n",
            "Speed: 1.4ms preprocess, 46.8ms inference, 4.0ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "0: 640x448 3 persons, 1 bicycle, 37.2ms\n",
            "Speed: 1.7ms preprocess, 37.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 640x448 4 persons, 1 bicycle, 7 cars, 5 traffic lights, 34.6ms\n",
            "Speed: 2.4ms preprocess, 34.6ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "0: 480x640 15 persons, 2 bicycles, 2 motorcycles, 1 backpack, 2 handbags, 1 suitcase, 41.6ms\n",
            "Speed: 1.3ms preprocess, 41.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 2 persons, 1 train, 2 skateboards, 35.5ms\n",
            "Speed: 1.3ms preprocess, 35.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 2 persons, 3 bicycles, 4 airplanes, 3 bottles, 1 chair, 36.5ms\n",
            "Speed: 2.1ms preprocess, 36.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 448x640 16 persons, 1 bicycle, 3 cars, 1 train, 1 truck, 35.1ms\n",
            "Speed: 2.0ms preprocess, 35.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "0: 480x640 3 persons, 2 bicycles, 3 cars, 1 backpack, 37.6ms\n",
            "Speed: 1.5ms preprocess, 37.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 384x640 5 persons, 1 bicycle, 9 benchs, 4 chairs, 34.2ms\n",
            "Speed: 1.1ms preprocess, 34.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 480x640 12 persons, 8 bicycles, 4 cars, 21 motorcycles, 1 bus, 1 truck, 1 traffic light, 1 umbrella, 36.4ms\n",
            "Speed: 1.4ms preprocess, 36.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 16 persons, 1 car, 1 backpack, 4 surfboards, 34.9ms\n",
            "Speed: 1.4ms preprocess, 34.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 3 bicycles, 2 cars, 1 skateboard, 37.8ms\n",
            "Speed: 1.4ms preprocess, 37.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 448x640 15 persons, 1 bicycle, 2 motorcycles, 1 backpack, 35.7ms\n",
            "Speed: 2.3ms preprocess, 35.7ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Evaluating detections...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.eval.detection:Evaluating detections...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 149/149 [9.3s elapsed, 0s remaining, 7.7 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 149/149 [9.3s elapsed, 0s remaining, 7.7 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing IoU sweep...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.eval.coco:Performing IoU sweep...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |█████████████████| 149/149 [7.2s elapsed, 0s remaining, 25.3 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 149/149 [7.2s elapsed, 0s remaining, 25.3 samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Average Precision (mAP):\n",
            "0.3866536614882825\n",
            "\n",
            "Detailed Evaluation Report:\n",
            "{'airplane': {'precision': 0.8, 'recall': 0.8, 'f1-score': 0.8000000000000002, 'support': 5}, 'apple': {'precision': 0.2857142857142857, 'recall': 0.3333333333333333, 'f1-score': 0.30769230769230765, 'support': 6}, 'backpack': {'precision': 0.6363636363636364, 'recall': 0.38181818181818183, 'f1-score': 0.47727272727272735, 'support': 55}, 'banana': {'precision': 0.75, 'recall': 0.3157894736842105, 'f1-score': 0.44444444444444436, 'support': 19}, 'bed': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'bench': {'precision': 0.6060606060606061, 'recall': 0.6896551724137931, 'f1-score': 0.6451612903225807, 'support': 29}, 'bicycle': {'precision': 0.7276119402985075, 'recall': 0.6151419558359621, 'f1-score': 0.6666666666666666, 'support': 317}, 'bird': {'precision': 1.0, 'recall': 0.2, 'f1-score': 0.33333333333333337, 'support': 15}, 'boat': {'precision': 0.8518518518518519, 'recall': 0.71875, 'f1-score': 0.7796610169491525, 'support': 32}, 'book': {'precision': 0.8333333333333334, 'recall': 0.22727272727272727, 'f1-score': 0.35714285714285715, 'support': 22}, 'bottle': {'precision': 0.6666666666666666, 'recall': 0.7272727272727273, 'f1-score': 0.6956521739130435, 'support': 11}, 'bowl': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'broccoli': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'bus': {'precision': 0.7941176470588235, 'recall': 0.7941176470588235, 'f1-score': 0.7941176470588235, 'support': 34}, 'cake': {'precision': 1.0, 'recall': 0.07692307692307693, 'f1-score': 0.14285714285714288, 'support': 13}, 'car': {'precision': 0.7914691943127962, 'recall': 0.7952380952380952, 'f1-score': 0.7933491686460806, 'support': 210}, 'cat': {'precision': 0.75, 'recall': 1.0, 'f1-score': 0.8571428571428571, 'support': 3}, 'cell phone': {'precision': 1.0, 'recall': 0.25, 'f1-score': 0.4, 'support': 8}, 'chair': {'precision': 0.7333333333333333, 'recall': 0.4782608695652174, 'f1-score': 0.5789473684210527, 'support': 69}, 'clock': {'precision': 0.5714285714285714, 'recall': 0.8, 'f1-score': 0.6666666666666666, 'support': 5}, 'couch': {'precision': 1.0, 'recall': 0.4, 'f1-score': 0.5714285714285715, 'support': 5}, 'cow': {'precision': 0.75, 'recall': 0.6666666666666666, 'f1-score': 0.7058823529411765, 'support': 9}, 'cup': {'precision': 0.9090909090909091, 'recall': 0.7142857142857143, 'f1-score': 0.8, 'support': 14}, 'dining table': {'precision': 0.4, 'recall': 0.2857142857142857, 'f1-score': 0.3333333333333333, 'support': 14}, 'dog': {'precision': 0.6923076923076923, 'recall': 0.75, 'f1-score': 0.7199999999999999, 'support': 12}, 'fire hydrant': {'precision': 1.0, 'recall': 0.8571428571428571, 'f1-score': 0.923076923076923, 'support': 7}, 'handbag': {'precision': 0.4883720930232558, 'recall': 0.3, 'f1-score': 0.37168141592920345, 'support': 70}, 'horse': {'precision': 0.8333333333333334, 'recall': 0.8333333333333334, 'f1-score': 0.8333333333333334, 'support': 6}, 'hot dog': {'precision': 1.0, 'recall': 0.6666666666666666, 'f1-score': 0.8, 'support': 6}, 'kite': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 1}, 'knife': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'laptop': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'microwave': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'motorcycle': {'precision': 0.6578947368421053, 'recall': 0.6329113924050633, 'f1-score': 0.6451612903225806, 'support': 79}, 'orange': {'precision': 1.0, 'recall': 0.2, 'f1-score': 0.33333333333333337, 'support': 5}, 'parking meter': {'precision': 0.75, 'recall': 0.6, 'f1-score': 0.6666666666666665, 'support': 5}, 'person': {'precision': 0.8882211538461539, 'recall': 0.8192904656319291, 'f1-score': 0.8523644752018456, 'support': 902}, 'pizza': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2}, 'potted plant': {'precision': 0.47368421052631576, 'recall': 0.6, 'f1-score': 0.5294117647058824, 'support': 15}, 'refrigerator': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'remote': {'precision': 0.3333333333333333, 'recall': 0.25, 'f1-score': 0.28571428571428575, 'support': 4}, 'sandwich': {'precision': 0.4, 'recall': 1.0, 'f1-score': 0.5714285714285715, 'support': 2}, 'scissors': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 2}, 'sheep': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'skateboard': {'precision': 0.8181818181818182, 'recall': 0.8571428571428571, 'f1-score': 0.8372093023255814, 'support': 21}, 'spoon': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 1}, 'sports ball': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1}, 'stop sign': {'precision': 0.3333333333333333, 'recall': 0.25, 'f1-score': 0.28571428571428575, 'support': 4}, 'suitcase': {'precision': 0.6666666666666666, 'recall': 0.47058823529411764, 'f1-score': 0.5517241379310345, 'support': 17}, 'surfboard': {'precision': 0.9166666666666666, 'recall': 0.9166666666666666, 'f1-score': 0.9166666666666666, 'support': 12}, 'teddy bear': {'precision': 0.5, 'recall': 1.0, 'f1-score': 0.6666666666666666, 'support': 1}, 'tie': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2}, 'traffic light': {'precision': 0.7073170731707317, 'recall': 0.5858585858585859, 'f1-score': 0.6408839779005524, 'support': 99}, 'train': {'precision': 0.75, 'recall': 1.0, 'f1-score': 0.8571428571428571, 'support': 9}, 'truck': {'precision': 0.625, 'recall': 0.5555555555555556, 'f1-score': 0.5882352941176471, 'support': 27}, 'tv': {'precision': 0.6, 'recall': 0.75, 'f1-score': 0.6666666666666665, 'support': 4}, 'umbrella': {'precision': 0.6764705882352942, 'recall': 0.696969696969697, 'f1-score': 0.6865671641791046, 'support': 33}, 'vase': {'precision': 0.5, 'recall': 0.25, 'f1-score': 0.3333333333333333, 'support': 8}, 'micro avg': {'precision': 0.7852077001013171, 'recall': 0.6864481842338352, 'f1-score': 0.7325141776937619, 'support': 2258}, 'macro avg': {'precision': 0.6115142185341385, 'recall': 0.5191787282715542, 'f1-score': 0.5266735805503996, 'support': 2258}, 'weighted avg': {'precision': 0.784749692415924, 'recall': 0.6864481842338352, 'f1-score': 0.7224900293521892, 'support': 2258}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2eFdt3TrfBzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUGtzeGuc1md"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}